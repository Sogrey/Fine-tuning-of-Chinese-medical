import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from peft import PeftModel
import gc
from datetime import datetime

# æ¨¡å‹è·¯å¾„é…ç½®
BASE_MODEL = "models/Qwen/Qwen3-4B"
LORA_MODEL = "models/Sogrey/Chinese-medical-lora"

class MedicalQA_16GB_Optimized:
    def __init__(self):
        """åˆå§‹åŒ–åŒ»ç–—QAç³»ç»Ÿï¼ˆ16GB GPUä¼˜åŒ–ç‰ˆï¼‰"""
        assert torch.cuda.is_available(), "éœ€è¦NVIDIA GPUæ”¯æŒ"
        assert torch.cuda.get_device_properties(0).total_memory >= 16*1024**3, "éœ€è¦è‡³å°‘16GBæ˜¾å­˜"
        
        self.device = "cuda"
        self._setup_environment()
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            BASE_MODEL,
            trust_remote_code=True,
            pad_token='<|extra_0|>'
        )
        self.model = None
        self._load_models()

    def _setup_environment(self):
        """é…ç½®è¿è¡Œç¯å¢ƒ"""
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.set_grad_enabled(False)
        gc.collect()
        torch.cuda.empty_cache()

    def _load_models(self):
        """åŠ è½½å®Œæ•´ç²¾åº¦æ¨¡å‹å’ŒLoRAé€‚é…å™¨"""
        print("ğŸ¦¾ æ­£åœ¨ä¸º16GB GPUåŠ è½½æ¨¡å‹...")
        
        try:
            # 1. åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆå…¨ç²¾åº¦ï¼‰
            base_model = AutoModelForCausalLM.from_pretrained(
                BASE_MODEL,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )

            # 2. åŠ è½½LoRAé€‚é…å™¨ï¼ˆåˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ï¼‰
            self.model = PeftModel.from_pretrained(
                base_model,
                LORA_MODEL,
                device_map="auto"
            ).eval()

            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å½“å‰æ˜¾å­˜ä½¿ç”¨ï¼š{torch.cuda.memory_allocated()/1024**3:.2f}GB")
            
        except Exception as e:
            self._cleanup()
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

    def stream_answer(self, question, max_length=1024):
        """æµå¼ç”Ÿæˆå›ç­”ï¼ˆæ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡ï¼‰"""
        try:
            inputs = self.tokenizer(question, return_tensors="pt", padding=True).to(self.device)
            
            # ä¼˜åŒ–ç”Ÿæˆé…ç½®ï¼ˆåˆ©ç”¨æ›´å¤§æ˜¾å­˜ï¼‰
            generation_config = GenerationConfig(
                max_new_tokens=max_length,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=20,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                bos_token_id=151643
            )
            
            # é«˜æ€§èƒ½ç”Ÿæˆï¼ˆä½¿ç”¨CUDAå›¾ä¼˜åŒ–ï¼‰
            with torch.cuda.amp.autocast(dtype=torch.bfloat16), torch.no_grad():
                output_stream = self.model.generate(
                    **inputs,
                    generation_config=generation_config,
                    streamer=None,
                    return_dict_in_generate=True,
                    output_scores=True,
                    use_cache=True  # å……åˆ†åˆ©ç”¨å¤§æ˜¾å­˜
                )
            
            # æµå¼è¾“å‡ºå¤„ç†
            full_text = ""
            for seq in output_stream.sequences:
                decoded = self.tokenizer.decode(seq, skip_special_tokens=True)
                new_text = decoded[len(question):].strip() if decoded.startswith(question) else decoded
                if new_text and new_text != full_text:
                    yield new_text[len(full_text):]
                    full_text = new_text
                    
        except Exception as e:
            raise RuntimeError(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
        finally:
            torch.cuda.empty_cache()

    def _cleanup(self):
        """èµ„æºæ¸…ç†"""
        try:
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
            torch.cuda.empty_cache()
            gc.collect()
        except:
            pass

    def __del__(self):
        self._cleanup()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    qa = None  # å…ˆåˆå§‹åŒ–ä¸ºNone
    try:
        print("=== åŒ»ç–—é—®ç­”ç³»ç»Ÿ (16GB GPUé«˜æ€§èƒ½ç‰ˆ) ===")
        print(f"åŸºç¡€æ¨¡å‹è·¯å¾„: {BASE_MODEL}")
        print(f"LoRAé€‚é…å™¨è·¯å¾„: {LORA_MODEL}")
        
        qa = MedicalQA_16GB_Optimized()
        
        while True:
            question = input("\næ‚£è€…æé—®: ").strip()
            if question.lower() in ['exit', 'quit', 'q']:
                break
                
            print("\nAIåŒ»ç”Ÿæ€è€ƒä¸­...", end="", flush=True)
            
            try:
                # æµå¼è¾“å‡º
                for chunk in qa.stream_answer(question):
                    print(chunk, end="", flush=True)
                print("\n")  # å›ç­”ç»“æŸåæ¢è¡Œ
                
            except Exception as e:
                print(f"\nâŒ ç”Ÿæˆé”™è¯¯: {str(e)}")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å·²é€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {str(e)}")
    finally:
        if qa is not None:  # æ›´å®‰å…¨çš„æ£€æŸ¥æ–¹å¼
            qa._cleanup()
