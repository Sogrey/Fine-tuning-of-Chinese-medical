### ä½¿ç”¨ Unsloth æ¡†æ¶å¯¹ Qwen3-4B æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ç¤ºä¾‹ä»£ç 
### æœ¬ä»£ç å¯ä»¥åœ¨å…è´¹çš„ Tesla T4 Google Colab å®ä¾‹ä¸Šè¿è¡Œ https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb


```python
!pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple
```

    Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
    Collecting modelscope
      Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8b/2b/3f7efc0538766ecfc082e47d8358abdfad764397665e8576e0c00edaf0c1/modelscope-1.30.0-py3-none-any.whl (5.9 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m5.9/5.9 MB[0m [31m20.3 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hRequirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from modelscope) (3.16.1)
    Requirement already satisfied: requests>=2.25 in /root/miniconda3/lib/python3.12/site-packages (from modelscope) (2.31.0)
    Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.12/site-packages (from modelscope) (69.5.1)
    Requirement already satisfied: tqdm>=4.64.0 in /root/miniconda3/lib/python3.12/site-packages (from modelscope) (4.66.2)
    Requirement already satisfied: urllib3>=1.26 in /root/miniconda3/lib/python3.12/site-packages (from modelscope) (2.1.0)
    Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.25->modelscope) (2.0.4)
    Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.25->modelscope) (3.7)
    Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.25->modelscope) (2024.2.2)
    Installing collected packages: modelscope
    Successfully installed modelscope-1.30.0
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
    [0m


```python
!pip install unsloth
```

    Looking in indexes: http://mirrors.aliyun.com/pypi/simple
    Collecting unsloth
      Downloading http://mirrors.aliyun.com/pypi/packages/54/54/54d63bb23e908d83fd1aa5fca00f49fa2a10ffa5f929b19183abb2b1f890/unsloth-2025.9.7-py3-none-any.whl (314 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m314.6/314.6 kB[0m [31m4.3 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hCollecting unsloth_zoo>=2025.9.9 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/87/52/f86f72c86e6e3003e810cdf4c7ec05aebe29a1081c618ee8dc832aeb3af9/unsloth_zoo-2025.9.9-py3-none-any.whl (233 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m233.9/233.9 kB[0m [31m5.0 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hRequirement already satisfied: torch>=2.4.0 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (2.5.1+cu124)
    Collecting xformers>=0.0.27.post2 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/ce/16/4ed2fbf7c20ba20565d9a1fc544c2bef39d409c8eca1949654c6ff9f8452/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m117.2/117.2 MB[0m [31m5.2 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting bitsandbytes (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/9c/40/91f1a5a694f434bc13cba160045fdc4e867032e627b001bf411048fefd9c/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m61.3/61.3 MB[0m [31m7.0 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hRequirement already satisfied: triton>=3.0.0 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (3.1.0)
    Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (23.2)
    Collecting tyro (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/1c/dc/453d69f41c135597c5a21a3be4835adcf02455a35579203c0f550c35daeb/tyro-0.9.32-py3-none-any.whl (132 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m132.5/132.5 kB[0m [31m18.7 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.55.4,>=4.51.3 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/fa/0a/8791a6ee0529c45f669566969e99b75e2ab20eb0bfee8794ce295c18bdad/transformers-4.55.4-py3-none-any.whl (11.3 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m11.3/11.3 MB[0m [31m7.3 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting datasets<4.0.0,>=3.4.1 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/20/34/a08b0ee99715eaba118cbe19a71f7b5e2425c2718ef96007c325944a1152/datasets-3.6.0-py3-none-any.whl (491 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m491.5/491.5 kB[0m [31m8.5 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hCollecting sentencepiece>=0.2.0 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/04/88/14f2f4a2b922d8b39be45bf63d79e6cd3a9b2f248b2fcb98a69b12af12f5/sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.4/1.4 MB[0m [31m3.2 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0mm
    [?25hRequirement already satisfied: tqdm in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (4.66.2)
    Requirement already satisfied: psutil in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (6.1.0)
    Requirement already satisfied: wheel>=0.42.0 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.43.0)
    Requirement already satisfied: numpy in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (2.1.3)
    Collecting accelerate>=0.34.1 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/5f/a0/d9ef19f780f319c21ee90ecfef4431cbeeca95bec7f14071785c17b6029b/accelerate-1.10.1-py3-none-any.whl (374 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m374.9/374.9 kB[0m [31m5.7 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hCollecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/0f/81/035cace9b8853df794db0499299273abef30de889602587efa2a95c7dccb/trl-0.23.0-py3-none-any.whl (564 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m564.7/564.7 kB[0m [31m956.4 kB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hCollecting peft!=0.11.0,>=0.7.1 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/49/fe/a2da1627aa9cb6310b6034598363bd26ac301c4a99d21f415b1b2855891e/peft-0.17.1-py3-none-any.whl (504 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m504.9/504.9 kB[0m [31m6.1 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hRequirement already satisfied: protobuf in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (5.28.3)
    Collecting huggingface_hub>=0.34.0 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/fe/85/a18508becfa01f1e4351b5e18651b06d210dbd96debccd48a452acccb901/huggingface_hub-0.35.0-py3-none-any.whl (563 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m563.4/563.4 kB[0m [31m4.8 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting hf_transfer (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/d6/d8/f87ea6f42456254b48915970ed98e993110521e9263472840174d32c880d/hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m3.6/3.6 MB[0m [31m7.0 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0mm
    [?25hCollecting diffusers (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/06/a7/c53f294f34d9e1584388721b3d7aa024ea1ac46e86d0c302fc3db40ed960/diffusers-0.35.1-py3-none-any.whl (4.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m4.1/4.1 MB[0m [31m7.3 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0mm
    [?25hRequirement already satisfied: torchvision in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.20.1+cu124)
    Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth) (6.0.2)
    Collecting safetensors>=0.4.3 (from accelerate>=0.34.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/fe/5d/5a514d7b88e310c8b146e2404e0dc161282e78634d9358975fd56dfd14be/safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m485.8/485.8 kB[0m [31m7.8 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hRequirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.16.1)
    Collecting pyarrow>=15.0.0 (from datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/ad/90/2660332eeb31303c13b653ea566a9918484b6e4d6b9d2d46879a33ab0622/pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m42.8/42.8 MB[0m [31m7.9 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting dill<0.3.9,>=0.3.0 (from datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m116.3/116.3 kB[0m [31m9.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting pandas (from datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/d3/a4/f7edcfa47e0a88cda0be8b068a5bae710bf264f867edfdf7b71584ace362/pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m12.0/12.0 MB[0m [31m8.8 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting requests>=2.32.2 (from datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/1e/db/4254e3eabe8020b458f1a747140d32277ec7a271daf1d235b70dc0b4e6e3/requests-2.32.5-py3-none-any.whl (64 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m64.7/64.7 kB[0m [31m25.3 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting tqdm (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m78.5/78.5 kB[0m [31m28.3 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting xxhash (from datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/11/a7/81dba5010f7e733de88af9555725146fc133be97ce36533867f4c7e75066/xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m194.4/194.4 kB[0m [31m15.5 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting multiprocess<0.70.17 (from datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/0a/7d/a988f258104dcd2ccf1ed40fdc97e26c4ac351eeaf81d76e266c52d84e2f/multiprocess-0.70.16-py312-none-any.whl (146 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m146.7/146.7 kB[0m [31m13.4 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2024.10.0)
    Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub>=0.34.0->unsloth) (4.12.2)
    Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.34.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/15/07/86397573efefff941e100367bbda0b21496ffcdb34db7ab51912994c32a2/hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m3.2/3.2 MB[0m [31m9.1 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0mm
    [?25hRequirement already satisfied: networkx in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (3.4.2)
    Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (3.1.4)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)
    Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (9.1.0.70)
    Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.5.8)
    Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (11.2.1.3)
    Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (10.3.5.147)
    Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (11.6.1.9)
    Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.3.1.170)
    Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (2.21.5)
    Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)
    Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)
    Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (69.5.1)
    Requirement already satisfied: sympy==1.13.1 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (1.13.1)
    Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)
    Collecting regex!=2019.12.17 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.55.4,>=4.51.3->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/35/9e/a91b50332a9750519320ed30ec378b74c996f6befe282cfa6bb6cea7e9fd/regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m802.0/802.0 kB[0m [31m9.7 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hCollecting tokenizers<0.22,>=0.21 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.55.4,>=4.51.3->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/f2/90/273b6c7ec78af547694eddeea9e05de771278bd20476525ab930cecaf7d8/tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m3.1/3.1 MB[0m [31m9.3 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0mm
    [?25hINFO: pip is looking at multiple versions of trl to determine which version is compatible with other requirements. This could take a while.
    Collecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/ed/89/b10851fcad71e9ff7a6e74c5981476a7a3aa4efc0e9ef24176eb859990c9/trl-0.22.2-py3-none-any.whl (544 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m544.8/544.8 kB[0m [31m11.9 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hCollecting torchao (from unsloth_zoo>=2025.9.9->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/f5/f3/5f652941cbc7d07f1d40bcfb7263919a4994f318e1cf7be45cf2b6107d46/torchao-0.13.0-1-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.9 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m6.9/6.9 MB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0mm
    [?25hCollecting packaging (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl (66 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m66.5/66.5 kB[0m [31m26.3 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting cut_cross_entropy (from unsloth_zoo>=2025.9.9->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/df/5f/62fdb048f84d19e2123b6bbd722fe09c8c79b4964c50094d1e979db808e2/cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)
    Requirement already satisfied: pillow in /root/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.9.9->unsloth) (11.0.0)
    Collecting msgspec (from unsloth_zoo>=2025.9.9->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/d0/ef/c5422ce8af73928d194a6606f8ae36e93a52fd5e8df5abd366903a5ca8da/msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m213.6/213.6 kB[0m [31m19.7 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting torch>=2.4.0 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/99/a8/6acf48d48838fb8fe480597d98a0668c2beb02ee4755cc136de92a0a956f/torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m887.9/887.9 MB[0m [31m5.3 MB/s[0m eta [36m0:00:00[0m00:01[0m00:03[0m
    [?25hCollecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/a2/09/77d55d46fd61b4a135c444fc97158ef34a095e5681d0a6c10b75bf356191/sympy-1.14.0-py3-none-any.whl (6.3 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m6.3/6.3 MB[0m [31m10.5 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/05/6b/32f747947df2da6994e999492ab306a903659555dddc0fbdeb9d71f75e52/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m88.0/88.0 MB[0m [31m10.0 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/0d/9b/a997b638fcd068ad6e4d53b8551a7d30fe8b404d6f1804abf1df69838932/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m954.8/954.8 kB[0m [31m12.3 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/f8/02/2adcaa145158bf1a8295d83591d22e4103dbfd821bcaf6f3f53151ca4ffa/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m10.2/10.2 MB[0m [31m11.8 MB/s[0m eta [36m0:00:00[0m00:01[0m0:01[0m
    [?25hCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/ba/51/e123d997aa098c61d029f76663dedbfb9bc8dcf8c60cbd6adbe42f76d049/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m706.8/706.8 MB[0m [31m6.4 MB/s[0m eta [36m0:00:00[0m00:01[0m00:02[0m
    [?25hCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/dc/61/e24b560ab2e2eaeb3c839129175fb330dfcfc29e5203196e5541a4c44682/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m594.3/594.3 MB[0m [31m7.4 MB/s[0m eta [36m0:00:00[0m00:01[0m00:02[0m
    [?25hCollecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/1f/13/ee4e00f30e676b66ae65b4f08cb5bcbb8392c03f54f2d5413ea99a5d1c80/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m193.1/193.1 MB[0m [31m11.2 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/fb/aa/6584b56dc84ebe9cf93226a5cde4d99080c8e90ab40f0c27bda7a0f29aa1/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m63.6/63.6 MB[0m [31m12.9 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/85/48/9a13d2975803e8cf2777d5ed57b87a0b6ca2cc795f9a4f59796a910bfb80/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m267.5/267.5 MB[0m [31m10.6 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/c2/f5/e1854cb2f2bcd4280c44736c93550cc300ff4b8c95ebe370d0aa7d2b473d/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m288.2/288.2 MB[0m [31m10.1 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/56/79/12978b96bd44274fe38b5dde5cfb660b1d114f70a65ef962bcbbed99b549/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m287.2/287.2 MB[0m [31m10.5 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-nccl-cu12==2.27.3 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m322.4/322.4 MB[0m [31m8.2 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/a2/eb/86626c1bbc2edb86323022371c39aa48df6fd8b0a1647bc274577f72e90b/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m90.0/90.0 kB[0m [31m22.7 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/f6/74/86a07f1d0f42998ca31312f998bd3b9a7eff7f52378f4f270c8679c77fb9/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m39.3/39.3 MB[0m [31m15.9 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/bb/fe/1bcba1dfbfb8d01be8d93f07bfc502c93fa23afa6fd5ab3fc7c1df71038a/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.2/1.2 MB[0m [31m19.1 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hCollecting triton>=3.0.0 (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/d0/66/b1eb52839f563623d185f0927eb3530ee4d5ffe9d377cdaf5346b306689e/triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m155.6/155.6 MB[0m [31m13.3 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting importlib_metadata (from diffusers->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/20/b0/36bd937216ec521246249be3bf9855081de4c5e06a0c9b4219dbeda50373/importlib_metadata-8.7.0-py3-none-any.whl (27 kB)
    INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.
    Collecting torchvision (from unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/e4/b5/3e580dcbc16f39a324f3dd71b90edbf02a42548ad44d2b4893cc92b1194b/torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m8.6/8.6 MB[0m [31m19.3 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
    [?25hCollecting docstring-parser>=0.15 (from tyro->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/55/e2/2537ebcff11c1ee1ff17d8d0b6f4db75873e3b0fb32c2d4a2ee31ecb310a/docstring_parser-0.17.0-py3-none-any.whl (36 kB)
    Collecting rich>=11.1.0 (from tyro->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/e3/30/3c4d035596d3cf444529e0b2953ad0466f6049528a879d27534700580395/rich-14.1.0-py3-none-any.whl (243 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m243.4/243.4 kB[0m [31m20.2 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting shtab>=1.5.6 (from tyro->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/74/03/3271b7bb470fbab4adf5bd30b0d32143909d96f3608d815b447357f47f2b/shtab-1.7.2-py3-none-any.whl (14 kB)
    Collecting typeguard>=4.0.0 (from tyro->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/1b/a9/e3aee762739c1d7528da1c3e06d518503f8b6c439c35549b53735ba52ead/typeguard-4.4.4-py3-none-any.whl (34 kB)
    Collecting typing-extensions>=3.7.4.3 (from huggingface_hub>=0.34.0->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/18/67/36e9267722cc04a6b9f15c7f3441c2363321a3ea07da7ae0c0707beb2a9c/typing_extensions-4.15.0-py3-none-any.whl (44 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m44.6/44.6 kB[0m [31m12.7 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/de/5e/3bf5acea47a96a28c121b167f5ef659cf71208b19e52a88cdfa5c37f1fcc/aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.7/1.7 MB[0m [31m19.8 MB/s[0m eta [36m0:00:00[0ma [36m0:00:01[0m
    [?25hRequirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.0.4)
    Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.7)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.1.0)
    Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2024.2.2)
    Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/94/54/e7d793b573f298e1c9013b8c4dade17d481164aa517d1d7148619c2cedbf/markdown_it_py-4.0.0-py3-none-any.whl (87 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m87.3/87.3 kB[0m [31m30.6 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth) (2.18.0)
    Collecting zipp>=3.20 (from importlib_metadata->diffusers->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/2e/54/647ade08bf0db230bfea292f893923872fd20be6ac6f53b2b936ba839d75/zipp-3.23.0-py3-none-any.whl (10 kB)
    Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)
    Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)
    Collecting pytz>=2020.1 (from pandas->datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl (509 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m509.2/509.2 kB[0m [31m12.1 MB/s[0m eta [36m0:00:00[0m00:01[0m
    [?25hCollecting tzdata>=2022.7 (from pandas->datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl (347 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m347.8/347.8 kB[0m [31m21.6 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)
    Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/fb/76/641ae371508676492379f16e2fa48f4e2c11741bd63c48be4b12a6b09cba/aiosignal-1.4.0-py3-none-any.whl (7.5 kB)
    Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (24.2.0)
    Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/8d/db/48421f62a6f77c553575201e89048e97198046b793f4a089c79a6e3268bd/frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m241.8/241.8 kB[0m [31m10.8 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/af/65/753a2d8b05daf496f4a9c367fe844e90a1b2cac78e2be2c844200d10cc4c/multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m256.1/256.1 kB[0m [31m24.6 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/37/7c/54fd5301ef38505ab235d98827207176a5c9b2aa61939b10a460ca53e123/propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m224.4/224.4 kB[0m [31m57.0 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/98/28/3ab7acc5b51f4434b181b0cee8f1f4b77a65919700a355fb3617f9488874/yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m355.6/355.6 kB[0m [31m28.5 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth)
      Downloading http://mirrors.aliyun.com/pypi/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl (10.0 kB)
    Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.16.0)
    Installing collected packages: torchao, pytz, nvidia-cusparselt-cu12, zipp, xxhash, tzdata, typing-extensions, triton, tqdm, sympy, shtab, sentencepiece, safetensors, requests, regex, pyarrow, propcache, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multidict, msgspec, mdurl, hf-xet, hf_transfer, frozenlist, docstring-parser, dill, aiohappyeyeballs, yarl, typeguard, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, markdown-it-py, importlib_metadata, huggingface_hub, aiosignal, tokenizers, rich, nvidia-cusolver-cu12, diffusers, aiohttp, tyro, transformers, torch, xformers, torchvision, datasets, cut_cross_entropy, bitsandbytes, accelerate, trl, peft, unsloth_zoo, unsloth
      Attempting uninstall: typing-extensions
        Found existing installation: typing_extensions 4.12.2
        Uninstalling typing_extensions-4.12.2:
          Successfully uninstalled typing_extensions-4.12.2
      Attempting uninstall: triton
        Found existing installation: triton 3.1.0
        Uninstalling triton-3.1.0:
          Successfully uninstalled triton-3.1.0
      Attempting uninstall: tqdm
        Found existing installation: tqdm 4.66.2
        Uninstalling tqdm-4.66.2:
          Successfully uninstalled tqdm-4.66.2
      Attempting uninstall: sympy
        Found existing installation: sympy 1.13.1
        Uninstalling sympy-1.13.1:
          Successfully uninstalled sympy-1.13.1
      Attempting uninstall: requests
        Found existing installation: requests 2.31.0
        Uninstalling requests-2.31.0:
          Successfully uninstalled requests-2.31.0
      Attempting uninstall: packaging
        Found existing installation: packaging 23.2
        Uninstalling packaging-23.2:
          Successfully uninstalled packaging-23.2
      Attempting uninstall: nvidia-nvtx-cu12
        Found existing installation: nvidia-nvtx-cu12 12.4.127
        Uninstalling nvidia-nvtx-cu12-12.4.127:
          Successfully uninstalled nvidia-nvtx-cu12-12.4.127
      Attempting uninstall: nvidia-nvjitlink-cu12
        Found existing installation: nvidia-nvjitlink-cu12 12.4.127
        Uninstalling nvidia-nvjitlink-cu12-12.4.127:
          Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127
      Attempting uninstall: nvidia-nccl-cu12
        Found existing installation: nvidia-nccl-cu12 2.21.5
        Uninstalling nvidia-nccl-cu12-2.21.5:
          Successfully uninstalled nvidia-nccl-cu12-2.21.5
      Attempting uninstall: nvidia-curand-cu12
        Found existing installation: nvidia-curand-cu12 10.3.5.147
        Uninstalling nvidia-curand-cu12-10.3.5.147:
          Successfully uninstalled nvidia-curand-cu12-10.3.5.147
      Attempting uninstall: nvidia-cuda-runtime-cu12
        Found existing installation: nvidia-cuda-runtime-cu12 12.4.127
        Uninstalling nvidia-cuda-runtime-cu12-12.4.127:
          Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127
      Attempting uninstall: nvidia-cuda-nvrtc-cu12
        Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127
        Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:
          Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127
      Attempting uninstall: nvidia-cuda-cupti-cu12
        Found existing installation: nvidia-cuda-cupti-cu12 12.4.127
        Uninstalling nvidia-cuda-cupti-cu12-12.4.127:
          Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127
      Attempting uninstall: nvidia-cublas-cu12
        Found existing installation: nvidia-cublas-cu12 12.4.5.8
        Uninstalling nvidia-cublas-cu12-12.4.5.8:
          Successfully uninstalled nvidia-cublas-cu12-12.4.5.8
      Attempting uninstall: nvidia-cusparse-cu12
        Found existing installation: nvidia-cusparse-cu12 12.3.1.170
        Uninstalling nvidia-cusparse-cu12-12.3.1.170:
          Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170
      Attempting uninstall: nvidia-cufft-cu12
        Found existing installation: nvidia-cufft-cu12 11.2.1.3
        Uninstalling nvidia-cufft-cu12-11.2.1.3:
          Successfully uninstalled nvidia-cufft-cu12-11.2.1.3
      Attempting uninstall: nvidia-cudnn-cu12
        Found existing installation: nvidia-cudnn-cu12 9.1.0.70
        Uninstalling nvidia-cudnn-cu12-9.1.0.70:
          Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70
      Attempting uninstall: nvidia-cusolver-cu12
        Found existing installation: nvidia-cusolver-cu12 11.6.1.9
        Uninstalling nvidia-cusolver-cu12-11.6.1.9:
          Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9
      Attempting uninstall: torch
        Found existing installation: torch 2.5.1+cu124
        Uninstalling torch-2.5.1+cu124:
          Successfully uninstalled torch-2.5.1+cu124
      Attempting uninstall: torchvision
        Found existing installation: torchvision 0.20.1+cu124
        Uninstalling torchvision-0.20.1+cu124:
          Successfully uninstalled torchvision-0.20.1+cu124
    Successfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 bitsandbytes-0.47.0 cut_cross_entropy-25.1.1 datasets-3.6.0 diffusers-0.35.1 dill-0.3.8 docstring-parser-0.17.0 frozenlist-1.7.0 hf-xet-1.1.10 hf_transfer-0.1.9 huggingface_hub-0.35.0 importlib_metadata-8.7.0 markdown-it-py-4.0.0 mdurl-0.1.2 msgspec-0.19.0 multidict-6.6.4 multiprocess-0.70.16 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 packaging-25.0 pandas-2.3.2 peft-0.17.1 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 regex-2025.9.18 requests-2.32.5 rich-14.1.0 safetensors-0.6.2 sentencepiece-0.2.1 shtab-1.7.2 sympy-1.14.0 tokenizers-0.21.4 torch-2.8.0 torchao-0.13.0 torchvision-0.23.0 tqdm-4.67.1 transformers-4.55.4 triton-3.4.0 trl-0.22.2 typeguard-4.4.4 typing-extensions-4.15.0 tyro-0.9.32 tzdata-2025.2 unsloth-2025.9.7 unsloth_zoo-2025.9.9 xformers-0.0.32.post2 xxhash-3.5.0 yarl-1.20.1 zipp-3.23.0
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
    [0m


```python
# å¯¼å…¥å¿…è¦çš„åº“
from unsloth import FastLanguageModel
```

    ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
    ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!



```python
# https://modelscope.cn/
# Step1ï¼Œä¸‹è½½ qwen3-4B
from modelscope import snapshot_download
model_dir = snapshot_download('Qwen/Qwen3-4B', cache_dir='../models')
```

    Downloading Model from https://www.modelscope.cn to directory: ../models/Qwen/Qwen3-4B


    2025-09-22 20:32:20,753 - modelscope - INFO - Got 13 files, start to download ...



    Processing 13 items:   0%|          | 0.00/13.0 [00:00<?, ?it/s]



    Downloading [LICENSE]:   0%|          | 0.00/11.1k [00:00<?, ?B/s]



    Downloading [model-00001-of-00003.safetensors]:   0%|          | 0.00/3.69G [00:00<?, ?B/s]



    Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]



    Downloading [model-00002-of-00003.safetensors]:   0%|          | 0.00/3.71G [00:00<?, ?B/s]



    Downloading [configuration.json]:   0%|          | 0.00/73.0 [00:00<?, ?B/s]



    Downloading [config.json]:   0%|          | 0.00/726 [00:00<?, ?B/s]



    Downloading [generation_config.json]:   0%|          | 0.00/239 [00:00<?, ?B/s]



    Downloading [model-00003-of-00003.safetensors]:   0%|          | 0.00/95.0M [00:00<?, ?B/s]



    Downloading [model.safetensors.index.json]:   0%|          | 0.00/32.0k [00:00<?, ?B/s]



    Downloading [README.md]:   0%|          | 0.00/16.5k [00:00<?, ?B/s]



    Downloading [tokenizer.json]:   0%|          | 0.00/10.9M [00:00<?, ?B/s]



    Downloading [tokenizer_config.json]:   0%|          | 0.00/9.50k [00:00<?, ?B/s]



    Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]


    2025-09-22 20:39:48,663 - modelscope - INFO - Download model 'Qwen/Qwen3-4B' successfully.



```python
import torch
print("PyTorch ç‰ˆæœ¬:", torch.__version__)
print("CUDA æ˜¯å¦å¯ç”¨:", torch.cuda.is_available())  # åº”è¾“å‡º True
print("PyTorch ç¼–è¯‘çš„ CUDA ç‰ˆæœ¬:", torch.version.cuda)  # åº”è¾“å‡º 12.4 ç›¸å…³ç‰ˆæœ¬

# æŸ¥çœ‹ PyTorch é…ç½®ä¿¡æ¯
print(torch.__config__.show())  # æ‰“å°è¯¦ç»†é…ç½®
print(torch.version.cuda)       # CUDA ç‰ˆæœ¬
print(torch.backends.cudnn.enabled)  # æ˜¯å¦å¯ç”¨ cuDNN
```

    PyTorch ç‰ˆæœ¬: 2.8.0+cu128
    CUDA æ˜¯å¦å¯ç”¨: True
    PyTorch ç¼–è¯‘çš„ CUDA ç‰ˆæœ¬: 12.8
    PyTorch built with:
      - GCC 13.3
      - C++ Version: 201703
      - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
      - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
      - OpenMP 201511 (a.k.a. OpenMP 4.5)
      - LAPACK is enabled (usually provided by MKL)
      - NNPACK is enabled
      - CPU capability usage: AVX512
      - CUDA Runtime 12.8
      - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
      - CuDNN 91.0.2  (built against CUDA 12.9)
        - Built with CuDNN 90.8
      - Magma 2.6.1
      - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=a1cb3cc05d46d198467bebbb6e8fba50a325d4e7, CUDA_VERSION=12.8, CUDNN_VERSION=9.8.0, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 
    
    12.8
    True



```python

import torch

# è®¾ç½®æ¨¡å‹å‚æ•°
max_seq_length = 2048  # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦ï¼Œæ”¯æŒ RoPE ç¼©æ”¾
dtype = None  # æ•°æ®ç±»å‹ï¼ŒNone è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ã€‚Tesla T4 ä½¿ç”¨ Float16ï¼ŒAmpere+ ä½¿ç”¨ Bfloat16
load_in_4bit = True  # ä½¿ç”¨ 4bit é‡åŒ–æ¥å‡å°‘å†…å­˜ä½¿ç”¨

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "../models/Qwen/Qwen3-4B",  # ä½¿ç”¨Qwen3-4Bæ¨¡å‹
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)
```

    ==((====))==  Unsloth 2025.9.7: Fast Qwen3 patching. Transformers: 4.55.4.
       \\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.
    O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0
    \        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
     "-____-"     Free license: http://github.com/unslothai/unsloth
    Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!



    Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]



```python
# æ·»åŠ LoRAé€‚é…å™¨ï¼Œåªéœ€è¦æ›´æ–°1-10%çš„å‚æ•°
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,  # LoRAç§©ï¼Œå»ºè®®ä½¿ç”¨8ã€16ã€32ã€64ã€128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],  # éœ€è¦åº”ç”¨LoRAçš„æ¨¡å—
    lora_alpha = 16,  # LoRAç¼©æ”¾å› å­
    lora_dropout = 0,  # LoRA dropoutç‡ï¼Œ0ä¸ºä¼˜åŒ–è®¾ç½®
    bias = "none",    # åç½®é¡¹è®¾ç½®ï¼Œnoneä¸ºä¼˜åŒ–è®¾ç½®
    use_gradient_checkpointing = "unsloth",  # ä½¿ç”¨unslothçš„æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œå¯å‡å°‘30%æ˜¾å­˜ä½¿ç”¨
    random_state = 3407,  # éšæœºç§å­
    use_rslora = False,  # æ˜¯å¦ä½¿ç”¨rank stabilized LoRA
    loftq_config = None,  # LoftQé…ç½®
)
```

    Unsloth 2025.9.7 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.


### æ•°æ®å‡†å¤‡


```python
import os
import pandas as pd
from datasets import Dataset

# å®šä¹‰åŒ»ç–—å¯¹è¯çš„æç¤ºæ¨¡æ¿
medical_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ‚£è€…çš„é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚

### é—®é¢˜ï¼š
{}

### å›ç­”ï¼š
{}"""

# è·å–ç»“æŸæ ‡è®°
EOS_TOKEN = tokenizer.eos_token

def read_csv_with_encoding(file_path):
    """å°è¯•ä½¿ç”¨ä¸åŒçš„ç¼–ç è¯»å–CSVæ–‡ä»¶"""
    encodings = ['gbk', 'gb2312', 'gb18030', 'utf-8']
    for encoding in encodings:
        try:
            return pd.read_csv(file_path, encoding=encoding)
        except UnicodeDecodeError:
            continue
    raise ValueError(f"æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶: {file_path}")

def load_medical_data(data_dir):
    """åŠ è½½åŒ»ç–—å¯¹è¯æ•°æ®"""
    data = []
    departments = {
        'IM_å†…ç§‘': 'å†…ç§‘',
        'Surgical_å¤–ç§‘': 'å¤–ç§‘',
        'Pediatric_å„¿ç§‘': 'å„¿ç§‘',
        'Oncology_è‚¿ç˜¤ç§‘': 'è‚¿ç˜¤ç§‘',
        'OAGD_å¦‡äº§ç§‘': 'å¦‡äº§ç§‘',
        'Andriatria_ç”·ç§‘': 'ç”·ç§‘'
    }
    
    # éå†æ‰€æœ‰ç§‘å®¤ç›®å½•
    for dept_dir, dept_name in departments.items():
        dept_path = os.path.join(data_dir, dept_dir)
        if not os.path.exists(dept_path):
            print(f"ç›®å½•ä¸å­˜åœ¨: {dept_path}")
            continue
            
        print(f"\nå¤„ç†{dept_name}æ•°æ®...")
        
        # è·å–è¯¥ç§‘å®¤ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = [f for f in os.listdir(dept_path) if f.endswith('.csv')]
        
        for csv_file in csv_files:
            file_path = os.path.join(dept_path, csv_file)
            print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {csv_file}")
            
            try:
                # è¯»å–CSVæ–‡ä»¶
                df = read_csv_with_encoding(file_path)
                
                # æ‰“å°åˆ—åï¼Œå¸®åŠ©è°ƒè¯•
                print(f"æ–‡ä»¶ {csv_file} çš„åˆ—å: {df.columns.tolist()}")
                
                # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
                for _, row in df.iterrows():
                    try:
                        # è·å–é—®é¢˜å’Œå›ç­”ï¼ˆå°è¯•ä¸åŒçš„åˆ—åï¼‰
                        question = None
                        answer = None
                        
                        # å°è¯•ä¸åŒçš„åˆ—å
                        if 'question' in row:
                            question = str(row['question']).strip()
                        elif 'é—®é¢˜' in row:
                            question = str(row['é—®é¢˜']).strip()
                        elif 'ask' in row:
                            question = str(row['ask']).strip()
                            
                        if 'answer' in row:
                            answer = str(row['answer']).strip()
                        elif 'å›ç­”' in row:
                            answer = str(row['å›ç­”']).strip()
                        elif 'response' in row:
                            answer = str(row['response']).strip()
                        
                        # è¿‡æ»¤æ— æ•ˆæ•°æ®
                        if not question or not answer:
                            continue
                            
                        # é™åˆ¶é•¿åº¦
                        if len(question) > 200 or len(answer) > 200:
                            continue
                            
                        # æ·»åŠ åˆ°æ•°æ®åˆ—è¡¨
                        data.append({
                            "instruction": "è¯·å›ç­”ä»¥ä¸‹åŒ»ç–—ç›¸å…³é—®é¢˜",
                            "input": question,
                            "output": answer
                        })
                        
                    except Exception as e:
                        print(f"å¤„ç†æ•°æ®è¡Œæ—¶å‡ºé”™: {e}")
                        continue
                        
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {csv_file} æ—¶å‡ºé”™: {e}")
                continue
    
    # éªŒè¯æ•°æ®
    if not data:
        raise ValueError("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®ï¼")
        
    print(f"\næˆåŠŸå¤„ç† {len(data)} æ¡æ•°æ®")
    return Dataset.from_list(data)

def formatting_prompts_func(examples):
    """æ ¼å¼åŒ–æç¤º"""
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = medical_prompt.format(input, output) + EOS_TOKEN
        texts.append(text)
    return {"text": texts}

# åŠ è½½åŒ»ç–—æ•°æ®é›†
dataset = load_medical_data("Chinese-medical-dialogue-data/Data_æ•°æ®")
dataset = dataset.map(formatting_prompts_func, batched=True)
```

    
    å¤„ç†å†…ç§‘æ•°æ®...
    æ­£åœ¨å¤„ç†æ–‡ä»¶: å†…ç§‘5000-33000.csv
    æ–‡ä»¶ å†…ç§‘5000-33000.csv çš„åˆ—å: ['department', 'title', 'ask', 'answer']
    
    å¤„ç†å¤–ç§‘æ•°æ®...
    æ­£åœ¨å¤„ç†æ–‡ä»¶: å¤–ç§‘5-14000.csv
    æ–‡ä»¶ å¤–ç§‘5-14000.csv çš„åˆ—å: ['department', 'title', 'ask', 'answer']
    
    å¤„ç†å„¿ç§‘æ•°æ®...
    æ­£åœ¨å¤„ç†æ–‡ä»¶: å„¿ç§‘5-14000.csv
    æ–‡ä»¶ å„¿ç§‘5-14000.csv çš„åˆ—å: ['department', 'title', 'ask', 'answer']
    
    å¤„ç†è‚¿ç˜¤ç§‘æ•°æ®...
    æ­£åœ¨å¤„ç†æ–‡ä»¶: è‚¿ç˜¤ç§‘5-10000.csv
    æ–‡ä»¶ è‚¿ç˜¤ç§‘5-10000.csv çš„åˆ—å: ['department', 'title', 'ask', 'answer']
    
    å¤„ç†å¦‡äº§ç§‘æ•°æ®...
    æ­£åœ¨å¤„ç†æ–‡ä»¶: å¦‡äº§ç§‘6-28000.csv
    æ–‡ä»¶ å¦‡äº§ç§‘6-28000.csv çš„åˆ—å: ['department', 'title', 'ask', 'answer']
    
    å¤„ç†ç”·ç§‘æ•°æ®...
    æ­£åœ¨å¤„ç†æ–‡ä»¶: ç”·ç§‘5-13000.csv
    æ–‡ä»¶ ç”·ç§‘5-13000.csv çš„åˆ—å: ['department', 'title', 'ask', 'answer']
    
    æˆåŠŸå¤„ç† 663658 æ¡æ•°æ®



    Map:   0%|          | 0/663658 [00:00<?, ? examples/s]


### æ¨¡å‹è®­ç»ƒ


```python
# è®¾ç½®è®­ç»ƒå‚æ•°å’Œè®­ç»ƒå™¨
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
        per_device_train_batch_size = 2,  # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps = 4,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        warmup_steps = 5,  # é¢„çƒ­æ­¥æ•°
        max_steps = 600,  # æœ€å¤§è®­ç»ƒæ­¥æ•°
        # max_steps = -1,  # ä¸ä½¿ç”¨max_steps
        num_train_epochs = 3,  # è®­ç»ƒ3ä¸ªepoch
        learning_rate = 2e-4,  # å­¦ä¹ ç‡
        fp16 = not is_bfloat16_supported(),  # æ˜¯å¦ä½¿ç”¨FP16
        bf16 = is_bfloat16_supported(),  # æ˜¯å¦ä½¿ç”¨BF16
        logging_steps = 1,  # æ—¥å¿—è®°å½•æ­¥æ•°
        optim = "adamw_8bit",  # ä¼˜åŒ–å™¨
        weight_decay = 0.01,  # æƒé‡è¡°å‡
        lr_scheduler_type = "linear",  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
        seed = 3407,  # éšæœºç§å­
        output_dir = "outputs",  # è¾“å‡ºç›®å½•
        report_to = "none",  # æŠ¥å‘Šæ–¹å¼
    )

# åˆ›å»ºSFTTrainerå®ä¾‹
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False,  # å¯¹äºçŸ­åºåˆ—å¯ä»¥è®¾ç½®ä¸ºTrueï¼Œè®­ç»ƒé€Ÿåº¦æå‡5å€
    args = training_args,
)
```


    Unsloth: Tokenizing ["text"] (num_proc=132):   0%|          | 0/663658 [00:00<?, ? examples/s]



```python
# æ˜¾ç¤ºå½“å‰GPUå†…å­˜çŠ¶æ€
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")
```

    GPU = NVIDIA GeForce RTX 4090. Max memory = 23.527 GB.
    3.826 GB of memory reserved.



```python
# å¼€å§‹è®­ç»ƒ
trainer_stats = trainer.train()
```

    ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
       \\   /|    Num examples = 663,658 | Num Epochs = 1 | Total steps = 600
    O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
    \        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
     "-____-"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)




    <div>

      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [600/600 15:08, Epoch 0/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1.547100</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.865300</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.816600</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.788600</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1.896500</td>
    </tr>
    <tr>
      <td>6</td>
      <td>1.607800</td>
    </tr>
    <tr>
      <td>7</td>
      <td>1.644100</td>
    </tr>
    <tr>
      <td>8</td>
      <td>1.746700</td>
    </tr>
    <tr>
      <td>9</td>
      <td>1.361400</td>
    </tr>
    <tr>
      <td>10</td>
      <td>1.542100</td>
    </tr>
    <tr>
      <td>11</td>
      <td>1.577600</td>
    </tr>
    <tr>
      <td>12</td>
      <td>1.543500</td>
    </tr>
    <tr>
      <td>13</td>
      <td>1.870800</td>
    </tr>
    <tr>
      <td>14</td>
      <td>1.607200</td>
    </tr>
    <tr>
      <td>15</td>
      <td>1.592500</td>
    </tr>
    <tr>
      <td>16</td>
      <td>1.423800</td>
    </tr>
    <tr>
      <td>17</td>
      <td>1.790300</td>
    </tr>
    <tr>
      <td>18</td>
      <td>1.893100</td>
    </tr>
    <tr>
      <td>19</td>
      <td>1.649300</td>
    </tr>
    <tr>
      <td>20</td>
      <td>1.883100</td>
    </tr>
    <tr>
      <td>21</td>
      <td>1.626700</td>
    </tr>
    <tr>
      <td>22</td>
      <td>1.382600</td>
    </tr>
    <tr>
      <td>23</td>
      <td>1.223600</td>
    </tr>
    <tr>
      <td>24</td>
      <td>1.726700</td>
    </tr>
    <tr>
      <td>25</td>
      <td>1.791500</td>
    </tr>
    <tr>
      <td>26</td>
      <td>1.599900</td>
    </tr>
    <tr>
      <td>27</td>
      <td>1.751100</td>
    </tr>
    <tr>
      <td>28</td>
      <td>1.362600</td>
    </tr>
    <tr>
      <td>29</td>
      <td>1.813200</td>
    </tr>
    <tr>
      <td>30</td>
      <td>1.526600</td>
    </tr>
    <tr>
      <td>31</td>
      <td>1.560800</td>
    </tr>
    <tr>
      <td>32</td>
      <td>1.699700</td>
    </tr>
    <tr>
      <td>33</td>
      <td>1.776100</td>
    </tr>
    <tr>
      <td>34</td>
      <td>1.864100</td>
    </tr>
    <tr>
      <td>35</td>
      <td>1.404200</td>
    </tr>
    <tr>
      <td>36</td>
      <td>1.725800</td>
    </tr>
    <tr>
      <td>37</td>
      <td>1.790700</td>
    </tr>
    <tr>
      <td>38</td>
      <td>1.666300</td>
    </tr>
    <tr>
      <td>39</td>
      <td>1.616600</td>
    </tr>
    <tr>
      <td>40</td>
      <td>1.731400</td>
    </tr>
    <tr>
      <td>41</td>
      <td>1.343500</td>
    </tr>
    <tr>
      <td>42</td>
      <td>1.697600</td>
    </tr>
    <tr>
      <td>43</td>
      <td>1.850800</td>
    </tr>
    <tr>
      <td>44</td>
      <td>1.806200</td>
    </tr>
    <tr>
      <td>45</td>
      <td>1.635400</td>
    </tr>
    <tr>
      <td>46</td>
      <td>1.723400</td>
    </tr>
    <tr>
      <td>47</td>
      <td>1.705500</td>
    </tr>
    <tr>
      <td>48</td>
      <td>1.552000</td>
    </tr>
    <tr>
      <td>49</td>
      <td>1.820400</td>
    </tr>
    <tr>
      <td>50</td>
      <td>1.780300</td>
    </tr>
    <tr>
      <td>51</td>
      <td>1.776400</td>
    </tr>
    <tr>
      <td>52</td>
      <td>1.699400</td>
    </tr>
    <tr>
      <td>53</td>
      <td>1.860800</td>
    </tr>
    <tr>
      <td>54</td>
      <td>1.631400</td>
    </tr>
    <tr>
      <td>55</td>
      <td>1.527800</td>
    </tr>
    <tr>
      <td>56</td>
      <td>2.004500</td>
    </tr>
    <tr>
      <td>57</td>
      <td>1.992000</td>
    </tr>
    <tr>
      <td>58</td>
      <td>1.399000</td>
    </tr>
    <tr>
      <td>59</td>
      <td>1.637300</td>
    </tr>
    <tr>
      <td>60</td>
      <td>1.806300</td>
    </tr>
    <tr>
      <td>61</td>
      <td>1.896500</td>
    </tr>
    <tr>
      <td>62</td>
      <td>1.878800</td>
    </tr>
    <tr>
      <td>63</td>
      <td>1.904200</td>
    </tr>
    <tr>
      <td>64</td>
      <td>1.529000</td>
    </tr>
    <tr>
      <td>65</td>
      <td>1.954400</td>
    </tr>
    <tr>
      <td>66</td>
      <td>1.804600</td>
    </tr>
    <tr>
      <td>67</td>
      <td>1.375500</td>
    </tr>
    <tr>
      <td>68</td>
      <td>1.868300</td>
    </tr>
    <tr>
      <td>69</td>
      <td>1.564200</td>
    </tr>
    <tr>
      <td>70</td>
      <td>1.523300</td>
    </tr>
    <tr>
      <td>71</td>
      <td>1.853800</td>
    </tr>
    <tr>
      <td>72</td>
      <td>1.947800</td>
    </tr>
    <tr>
      <td>73</td>
      <td>1.679800</td>
    </tr>
    <tr>
      <td>74</td>
      <td>1.720900</td>
    </tr>
    <tr>
      <td>75</td>
      <td>1.651700</td>
    </tr>
    <tr>
      <td>76</td>
      <td>1.432900</td>
    </tr>
    <tr>
      <td>77</td>
      <td>1.515900</td>
    </tr>
    <tr>
      <td>78</td>
      <td>1.882200</td>
    </tr>
    <tr>
      <td>79</td>
      <td>1.496700</td>
    </tr>
    <tr>
      <td>80</td>
      <td>1.507300</td>
    </tr>
    <tr>
      <td>81</td>
      <td>1.825800</td>
    </tr>
    <tr>
      <td>82</td>
      <td>1.914600</td>
    </tr>
    <tr>
      <td>83</td>
      <td>1.778900</td>
    </tr>
    <tr>
      <td>84</td>
      <td>1.711400</td>
    </tr>
    <tr>
      <td>85</td>
      <td>1.734300</td>
    </tr>
    <tr>
      <td>86</td>
      <td>1.785800</td>
    </tr>
    <tr>
      <td>87</td>
      <td>1.838100</td>
    </tr>
    <tr>
      <td>88</td>
      <td>1.759400</td>
    </tr>
    <tr>
      <td>89</td>
      <td>1.865000</td>
    </tr>
    <tr>
      <td>90</td>
      <td>1.969100</td>
    </tr>
    <tr>
      <td>91</td>
      <td>1.320700</td>
    </tr>
    <tr>
      <td>92</td>
      <td>1.765400</td>
    </tr>
    <tr>
      <td>93</td>
      <td>1.581100</td>
    </tr>
    <tr>
      <td>94</td>
      <td>1.748600</td>
    </tr>
    <tr>
      <td>95</td>
      <td>1.470000</td>
    </tr>
    <tr>
      <td>96</td>
      <td>1.915900</td>
    </tr>
    <tr>
      <td>97</td>
      <td>1.838100</td>
    </tr>
    <tr>
      <td>98</td>
      <td>1.884500</td>
    </tr>
    <tr>
      <td>99</td>
      <td>1.468000</td>
    </tr>
    <tr>
      <td>100</td>
      <td>1.371000</td>
    </tr>
    <tr>
      <td>101</td>
      <td>1.523700</td>
    </tr>
    <tr>
      <td>102</td>
      <td>1.896500</td>
    </tr>
    <tr>
      <td>103</td>
      <td>1.795000</td>
    </tr>
    <tr>
      <td>104</td>
      <td>1.901800</td>
    </tr>
    <tr>
      <td>105</td>
      <td>1.982600</td>
    </tr>
    <tr>
      <td>106</td>
      <td>1.733000</td>
    </tr>
    <tr>
      <td>107</td>
      <td>1.557700</td>
    </tr>
    <tr>
      <td>108</td>
      <td>1.415000</td>
    </tr>
    <tr>
      <td>109</td>
      <td>1.894600</td>
    </tr>
    <tr>
      <td>110</td>
      <td>1.626100</td>
    </tr>
    <tr>
      <td>111</td>
      <td>1.946300</td>
    </tr>
    <tr>
      <td>112</td>
      <td>1.724800</td>
    </tr>
    <tr>
      <td>113</td>
      <td>1.997100</td>
    </tr>
    <tr>
      <td>114</td>
      <td>2.146500</td>
    </tr>
    <tr>
      <td>115</td>
      <td>1.869300</td>
    </tr>
    <tr>
      <td>116</td>
      <td>1.850500</td>
    </tr>
    <tr>
      <td>117</td>
      <td>1.676000</td>
    </tr>
    <tr>
      <td>118</td>
      <td>1.900000</td>
    </tr>
    <tr>
      <td>119</td>
      <td>1.741600</td>
    </tr>
    <tr>
      <td>120</td>
      <td>1.329500</td>
    </tr>
    <tr>
      <td>121</td>
      <td>1.896000</td>
    </tr>
    <tr>
      <td>122</td>
      <td>1.993700</td>
    </tr>
    <tr>
      <td>123</td>
      <td>1.470400</td>
    </tr>
    <tr>
      <td>124</td>
      <td>1.525900</td>
    </tr>
    <tr>
      <td>125</td>
      <td>2.033700</td>
    </tr>
    <tr>
      <td>126</td>
      <td>1.657400</td>
    </tr>
    <tr>
      <td>127</td>
      <td>1.588700</td>
    </tr>
    <tr>
      <td>128</td>
      <td>1.707600</td>
    </tr>
    <tr>
      <td>129</td>
      <td>1.440500</td>
    </tr>
    <tr>
      <td>130</td>
      <td>1.917000</td>
    </tr>
    <tr>
      <td>131</td>
      <td>1.801500</td>
    </tr>
    <tr>
      <td>132</td>
      <td>1.820700</td>
    </tr>
    <tr>
      <td>133</td>
      <td>1.636800</td>
    </tr>
    <tr>
      <td>134</td>
      <td>1.649200</td>
    </tr>
    <tr>
      <td>135</td>
      <td>1.813600</td>
    </tr>
    <tr>
      <td>136</td>
      <td>1.674200</td>
    </tr>
    <tr>
      <td>137</td>
      <td>1.548000</td>
    </tr>
    <tr>
      <td>138</td>
      <td>1.652500</td>
    </tr>
    <tr>
      <td>139</td>
      <td>1.665800</td>
    </tr>
    <tr>
      <td>140</td>
      <td>1.656900</td>
    </tr>
    <tr>
      <td>141</td>
      <td>2.027400</td>
    </tr>
    <tr>
      <td>142</td>
      <td>1.762400</td>
    </tr>
    <tr>
      <td>143</td>
      <td>1.486800</td>
    </tr>
    <tr>
      <td>144</td>
      <td>1.661100</td>
    </tr>
    <tr>
      <td>145</td>
      <td>1.510500</td>
    </tr>
    <tr>
      <td>146</td>
      <td>1.801600</td>
    </tr>
    <tr>
      <td>147</td>
      <td>1.921500</td>
    </tr>
    <tr>
      <td>148</td>
      <td>1.495700</td>
    </tr>
    <tr>
      <td>149</td>
      <td>1.774500</td>
    </tr>
    <tr>
      <td>150</td>
      <td>1.778500</td>
    </tr>
    <tr>
      <td>151</td>
      <td>1.861900</td>
    </tr>
    <tr>
      <td>152</td>
      <td>1.360900</td>
    </tr>
    <tr>
      <td>153</td>
      <td>1.412400</td>
    </tr>
    <tr>
      <td>154</td>
      <td>1.794200</td>
    </tr>
    <tr>
      <td>155</td>
      <td>1.932200</td>
    </tr>
    <tr>
      <td>156</td>
      <td>1.576900</td>
    </tr>
    <tr>
      <td>157</td>
      <td>1.614900</td>
    </tr>
    <tr>
      <td>158</td>
      <td>1.567600</td>
    </tr>
    <tr>
      <td>159</td>
      <td>2.070600</td>
    </tr>
    <tr>
      <td>160</td>
      <td>1.821300</td>
    </tr>
    <tr>
      <td>161</td>
      <td>1.767300</td>
    </tr>
    <tr>
      <td>162</td>
      <td>1.926500</td>
    </tr>
    <tr>
      <td>163</td>
      <td>1.518100</td>
    </tr>
    <tr>
      <td>164</td>
      <td>1.696500</td>
    </tr>
    <tr>
      <td>165</td>
      <td>1.901500</td>
    </tr>
    <tr>
      <td>166</td>
      <td>1.825500</td>
    </tr>
    <tr>
      <td>167</td>
      <td>2.015800</td>
    </tr>
    <tr>
      <td>168</td>
      <td>1.655300</td>
    </tr>
    <tr>
      <td>169</td>
      <td>1.873700</td>
    </tr>
    <tr>
      <td>170</td>
      <td>1.772400</td>
    </tr>
    <tr>
      <td>171</td>
      <td>1.881300</td>
    </tr>
    <tr>
      <td>172</td>
      <td>1.452800</td>
    </tr>
    <tr>
      <td>173</td>
      <td>1.762400</td>
    </tr>
    <tr>
      <td>174</td>
      <td>1.559200</td>
    </tr>
    <tr>
      <td>175</td>
      <td>2.023800</td>
    </tr>
    <tr>
      <td>176</td>
      <td>1.887500</td>
    </tr>
    <tr>
      <td>177</td>
      <td>1.845600</td>
    </tr>
    <tr>
      <td>178</td>
      <td>1.751200</td>
    </tr>
    <tr>
      <td>179</td>
      <td>1.930700</td>
    </tr>
    <tr>
      <td>180</td>
      <td>1.635700</td>
    </tr>
    <tr>
      <td>181</td>
      <td>1.890700</td>
    </tr>
    <tr>
      <td>182</td>
      <td>1.643800</td>
    </tr>
    <tr>
      <td>183</td>
      <td>1.927300</td>
    </tr>
    <tr>
      <td>184</td>
      <td>1.605500</td>
    </tr>
    <tr>
      <td>185</td>
      <td>1.991700</td>
    </tr>
    <tr>
      <td>186</td>
      <td>1.261700</td>
    </tr>
    <tr>
      <td>187</td>
      <td>1.590800</td>
    </tr>
    <tr>
      <td>188</td>
      <td>1.702000</td>
    </tr>
    <tr>
      <td>189</td>
      <td>1.859300</td>
    </tr>
    <tr>
      <td>190</td>
      <td>1.254200</td>
    </tr>
    <tr>
      <td>191</td>
      <td>1.435700</td>
    </tr>
    <tr>
      <td>192</td>
      <td>1.817000</td>
    </tr>
    <tr>
      <td>193</td>
      <td>1.587300</td>
    </tr>
    <tr>
      <td>194</td>
      <td>1.653600</td>
    </tr>
    <tr>
      <td>195</td>
      <td>1.710800</td>
    </tr>
    <tr>
      <td>196</td>
      <td>1.632600</td>
    </tr>
    <tr>
      <td>197</td>
      <td>1.710700</td>
    </tr>
    <tr>
      <td>198</td>
      <td>2.085800</td>
    </tr>
    <tr>
      <td>199</td>
      <td>1.783200</td>
    </tr>
    <tr>
      <td>200</td>
      <td>1.622900</td>
    </tr>
    <tr>
      <td>201</td>
      <td>1.673400</td>
    </tr>
    <tr>
      <td>202</td>
      <td>1.554900</td>
    </tr>
    <tr>
      <td>203</td>
      <td>1.812200</td>
    </tr>
    <tr>
      <td>204</td>
      <td>1.776000</td>
    </tr>
    <tr>
      <td>205</td>
      <td>1.380100</td>
    </tr>
    <tr>
      <td>206</td>
      <td>1.727500</td>
    </tr>
    <tr>
      <td>207</td>
      <td>2.030000</td>
    </tr>
    <tr>
      <td>208</td>
      <td>1.718100</td>
    </tr>
    <tr>
      <td>209</td>
      <td>1.777100</td>
    </tr>
    <tr>
      <td>210</td>
      <td>1.685600</td>
    </tr>
    <tr>
      <td>211</td>
      <td>1.538000</td>
    </tr>
    <tr>
      <td>212</td>
      <td>1.802900</td>
    </tr>
    <tr>
      <td>213</td>
      <td>1.788400</td>
    </tr>
    <tr>
      <td>214</td>
      <td>1.457600</td>
    </tr>
    <tr>
      <td>215</td>
      <td>1.995000</td>
    </tr>
    <tr>
      <td>216</td>
      <td>1.807300</td>
    </tr>
    <tr>
      <td>217</td>
      <td>1.733300</td>
    </tr>
    <tr>
      <td>218</td>
      <td>1.725500</td>
    </tr>
    <tr>
      <td>219</td>
      <td>1.873200</td>
    </tr>
    <tr>
      <td>220</td>
      <td>1.537900</td>
    </tr>
    <tr>
      <td>221</td>
      <td>1.768100</td>
    </tr>
    <tr>
      <td>222</td>
      <td>1.596000</td>
    </tr>
    <tr>
      <td>223</td>
      <td>1.496600</td>
    </tr>
    <tr>
      <td>224</td>
      <td>1.889000</td>
    </tr>
    <tr>
      <td>225</td>
      <td>1.374100</td>
    </tr>
    <tr>
      <td>226</td>
      <td>1.655700</td>
    </tr>
    <tr>
      <td>227</td>
      <td>2.015500</td>
    </tr>
    <tr>
      <td>228</td>
      <td>1.648900</td>
    </tr>
    <tr>
      <td>229</td>
      <td>1.587000</td>
    </tr>
    <tr>
      <td>230</td>
      <td>1.688700</td>
    </tr>
    <tr>
      <td>231</td>
      <td>1.631100</td>
    </tr>
    <tr>
      <td>232</td>
      <td>1.880700</td>
    </tr>
    <tr>
      <td>233</td>
      <td>1.534900</td>
    </tr>
    <tr>
      <td>234</td>
      <td>1.853500</td>
    </tr>
    <tr>
      <td>235</td>
      <td>1.096500</td>
    </tr>
    <tr>
      <td>236</td>
      <td>1.917600</td>
    </tr>
    <tr>
      <td>237</td>
      <td>1.803600</td>
    </tr>
    <tr>
      <td>238</td>
      <td>1.855400</td>
    </tr>
    <tr>
      <td>239</td>
      <td>1.487000</td>
    </tr>
    <tr>
      <td>240</td>
      <td>2.106900</td>
    </tr>
    <tr>
      <td>241</td>
      <td>1.845100</td>
    </tr>
    <tr>
      <td>242</td>
      <td>1.861800</td>
    </tr>
    <tr>
      <td>243</td>
      <td>1.762600</td>
    </tr>
    <tr>
      <td>244</td>
      <td>2.129000</td>
    </tr>
    <tr>
      <td>245</td>
      <td>1.651300</td>
    </tr>
    <tr>
      <td>246</td>
      <td>1.864100</td>
    </tr>
    <tr>
      <td>247</td>
      <td>1.731600</td>
    </tr>
    <tr>
      <td>248</td>
      <td>1.848500</td>
    </tr>
    <tr>
      <td>249</td>
      <td>1.777600</td>
    </tr>
    <tr>
      <td>250</td>
      <td>1.963500</td>
    </tr>
    <tr>
      <td>251</td>
      <td>2.060500</td>
    </tr>
    <tr>
      <td>252</td>
      <td>1.979000</td>
    </tr>
    <tr>
      <td>253</td>
      <td>1.884900</td>
    </tr>
    <tr>
      <td>254</td>
      <td>1.848200</td>
    </tr>
    <tr>
      <td>255</td>
      <td>1.968800</td>
    </tr>
    <tr>
      <td>256</td>
      <td>2.055800</td>
    </tr>
    <tr>
      <td>257</td>
      <td>2.015500</td>
    </tr>
    <tr>
      <td>258</td>
      <td>2.215500</td>
    </tr>
    <tr>
      <td>259</td>
      <td>2.059400</td>
    </tr>
    <tr>
      <td>260</td>
      <td>2.175100</td>
    </tr>
    <tr>
      <td>261</td>
      <td>1.879100</td>
    </tr>
    <tr>
      <td>262</td>
      <td>2.072600</td>
    </tr>
    <tr>
      <td>263</td>
      <td>1.914700</td>
    </tr>
    <tr>
      <td>264</td>
      <td>2.189000</td>
    </tr>
    <tr>
      <td>265</td>
      <td>1.928700</td>
    </tr>
    <tr>
      <td>266</td>
      <td>2.053300</td>
    </tr>
    <tr>
      <td>267</td>
      <td>2.113900</td>
    </tr>
    <tr>
      <td>268</td>
      <td>2.180500</td>
    </tr>
    <tr>
      <td>269</td>
      <td>2.129000</td>
    </tr>
    <tr>
      <td>270</td>
      <td>1.772300</td>
    </tr>
    <tr>
      <td>271</td>
      <td>1.810100</td>
    </tr>
    <tr>
      <td>272</td>
      <td>1.901800</td>
    </tr>
    <tr>
      <td>273</td>
      <td>1.913500</td>
    </tr>
    <tr>
      <td>274</td>
      <td>1.616200</td>
    </tr>
    <tr>
      <td>275</td>
      <td>1.849000</td>
    </tr>
    <tr>
      <td>276</td>
      <td>2.230400</td>
    </tr>
    <tr>
      <td>277</td>
      <td>2.080500</td>
    </tr>
    <tr>
      <td>278</td>
      <td>2.131800</td>
    </tr>
    <tr>
      <td>279</td>
      <td>1.852000</td>
    </tr>
    <tr>
      <td>280</td>
      <td>1.949100</td>
    </tr>
    <tr>
      <td>281</td>
      <td>1.931000</td>
    </tr>
    <tr>
      <td>282</td>
      <td>1.811600</td>
    </tr>
    <tr>
      <td>283</td>
      <td>2.034300</td>
    </tr>
    <tr>
      <td>284</td>
      <td>2.144700</td>
    </tr>
    <tr>
      <td>285</td>
      <td>2.350900</td>
    </tr>
    <tr>
      <td>286</td>
      <td>1.858300</td>
    </tr>
    <tr>
      <td>287</td>
      <td>2.329300</td>
    </tr>
    <tr>
      <td>288</td>
      <td>1.822500</td>
    </tr>
    <tr>
      <td>289</td>
      <td>2.291200</td>
    </tr>
    <tr>
      <td>290</td>
      <td>1.958900</td>
    </tr>
    <tr>
      <td>291</td>
      <td>2.210600</td>
    </tr>
    <tr>
      <td>292</td>
      <td>1.836300</td>
    </tr>
    <tr>
      <td>293</td>
      <td>1.694500</td>
    </tr>
    <tr>
      <td>294</td>
      <td>1.919900</td>
    </tr>
    <tr>
      <td>295</td>
      <td>2.116300</td>
    </tr>
    <tr>
      <td>296</td>
      <td>1.801500</td>
    </tr>
    <tr>
      <td>297</td>
      <td>1.871900</td>
    </tr>
    <tr>
      <td>298</td>
      <td>2.157600</td>
    </tr>
    <tr>
      <td>299</td>
      <td>2.024400</td>
    </tr>
    <tr>
      <td>300</td>
      <td>1.838300</td>
    </tr>
    <tr>
      <td>301</td>
      <td>2.037600</td>
    </tr>
    <tr>
      <td>302</td>
      <td>1.888000</td>
    </tr>
    <tr>
      <td>303</td>
      <td>1.752900</td>
    </tr>
    <tr>
      <td>304</td>
      <td>1.815500</td>
    </tr>
    <tr>
      <td>305</td>
      <td>2.110900</td>
    </tr>
    <tr>
      <td>306</td>
      <td>1.893600</td>
    </tr>
    <tr>
      <td>307</td>
      <td>1.456100</td>
    </tr>
    <tr>
      <td>308</td>
      <td>1.858000</td>
    </tr>
    <tr>
      <td>309</td>
      <td>1.808700</td>
    </tr>
    <tr>
      <td>310</td>
      <td>1.688400</td>
    </tr>
    <tr>
      <td>311</td>
      <td>2.019400</td>
    </tr>
    <tr>
      <td>312</td>
      <td>1.962900</td>
    </tr>
    <tr>
      <td>313</td>
      <td>2.014600</td>
    </tr>
    <tr>
      <td>314</td>
      <td>2.016900</td>
    </tr>
    <tr>
      <td>315</td>
      <td>1.724600</td>
    </tr>
    <tr>
      <td>316</td>
      <td>2.184600</td>
    </tr>
    <tr>
      <td>317</td>
      <td>2.059000</td>
    </tr>
    <tr>
      <td>318</td>
      <td>2.125100</td>
    </tr>
    <tr>
      <td>319</td>
      <td>1.639800</td>
    </tr>
    <tr>
      <td>320</td>
      <td>2.093700</td>
    </tr>
    <tr>
      <td>321</td>
      <td>2.028400</td>
    </tr>
    <tr>
      <td>322</td>
      <td>1.921000</td>
    </tr>
    <tr>
      <td>323</td>
      <td>1.611400</td>
    </tr>
    <tr>
      <td>324</td>
      <td>2.097400</td>
    </tr>
    <tr>
      <td>325</td>
      <td>2.197400</td>
    </tr>
    <tr>
      <td>326</td>
      <td>1.794900</td>
    </tr>
    <tr>
      <td>327</td>
      <td>2.114200</td>
    </tr>
    <tr>
      <td>328</td>
      <td>2.274700</td>
    </tr>
    <tr>
      <td>329</td>
      <td>1.934700</td>
    </tr>
    <tr>
      <td>330</td>
      <td>2.072700</td>
    </tr>
    <tr>
      <td>331</td>
      <td>2.188700</td>
    </tr>
    <tr>
      <td>332</td>
      <td>2.180900</td>
    </tr>
    <tr>
      <td>333</td>
      <td>2.403000</td>
    </tr>
    <tr>
      <td>334</td>
      <td>2.063200</td>
    </tr>
    <tr>
      <td>335</td>
      <td>2.135400</td>
    </tr>
    <tr>
      <td>336</td>
      <td>1.593300</td>
    </tr>
    <tr>
      <td>337</td>
      <td>2.121900</td>
    </tr>
    <tr>
      <td>338</td>
      <td>1.552600</td>
    </tr>
    <tr>
      <td>339</td>
      <td>2.028100</td>
    </tr>
    <tr>
      <td>340</td>
      <td>1.969800</td>
    </tr>
    <tr>
      <td>341</td>
      <td>2.442300</td>
    </tr>
    <tr>
      <td>342</td>
      <td>1.598700</td>
    </tr>
    <tr>
      <td>343</td>
      <td>2.181300</td>
    </tr>
    <tr>
      <td>344</td>
      <td>2.175400</td>
    </tr>
    <tr>
      <td>345</td>
      <td>2.143000</td>
    </tr>
    <tr>
      <td>346</td>
      <td>1.711900</td>
    </tr>
    <tr>
      <td>347</td>
      <td>2.421200</td>
    </tr>
    <tr>
      <td>348</td>
      <td>2.003200</td>
    </tr>
    <tr>
      <td>349</td>
      <td>2.230500</td>
    </tr>
    <tr>
      <td>350</td>
      <td>1.924400</td>
    </tr>
    <tr>
      <td>351</td>
      <td>2.456500</td>
    </tr>
    <tr>
      <td>352</td>
      <td>2.142900</td>
    </tr>
    <tr>
      <td>353</td>
      <td>2.066500</td>
    </tr>
    <tr>
      <td>354</td>
      <td>2.341500</td>
    </tr>
    <tr>
      <td>355</td>
      <td>1.975500</td>
    </tr>
    <tr>
      <td>356</td>
      <td>2.036100</td>
    </tr>
    <tr>
      <td>357</td>
      <td>2.169500</td>
    </tr>
    <tr>
      <td>358</td>
      <td>1.953700</td>
    </tr>
    <tr>
      <td>359</td>
      <td>2.097700</td>
    </tr>
    <tr>
      <td>360</td>
      <td>2.568600</td>
    </tr>
    <tr>
      <td>361</td>
      <td>2.224100</td>
    </tr>
    <tr>
      <td>362</td>
      <td>2.040100</td>
    </tr>
    <tr>
      <td>363</td>
      <td>2.394600</td>
    </tr>
    <tr>
      <td>364</td>
      <td>1.790300</td>
    </tr>
    <tr>
      <td>365</td>
      <td>2.150400</td>
    </tr>
    <tr>
      <td>366</td>
      <td>2.045400</td>
    </tr>
    <tr>
      <td>367</td>
      <td>2.109500</td>
    </tr>
    <tr>
      <td>368</td>
      <td>1.937200</td>
    </tr>
    <tr>
      <td>369</td>
      <td>2.219500</td>
    </tr>
    <tr>
      <td>370</td>
      <td>2.166000</td>
    </tr>
    <tr>
      <td>371</td>
      <td>1.888700</td>
    </tr>
    <tr>
      <td>372</td>
      <td>2.163800</td>
    </tr>
    <tr>
      <td>373</td>
      <td>2.365800</td>
    </tr>
    <tr>
      <td>374</td>
      <td>1.907800</td>
    </tr>
    <tr>
      <td>375</td>
      <td>2.049400</td>
    </tr>
    <tr>
      <td>376</td>
      <td>1.815000</td>
    </tr>
    <tr>
      <td>377</td>
      <td>1.994300</td>
    </tr>
    <tr>
      <td>378</td>
      <td>2.267300</td>
    </tr>
    <tr>
      <td>379</td>
      <td>1.980100</td>
    </tr>
    <tr>
      <td>380</td>
      <td>1.987000</td>
    </tr>
    <tr>
      <td>381</td>
      <td>1.561700</td>
    </tr>
    <tr>
      <td>382</td>
      <td>1.807600</td>
    </tr>
    <tr>
      <td>383</td>
      <td>2.091800</td>
    </tr>
    <tr>
      <td>384</td>
      <td>2.177500</td>
    </tr>
    <tr>
      <td>385</td>
      <td>2.030900</td>
    </tr>
    <tr>
      <td>386</td>
      <td>2.062800</td>
    </tr>
    <tr>
      <td>387</td>
      <td>2.231700</td>
    </tr>
    <tr>
      <td>388</td>
      <td>1.925600</td>
    </tr>
    <tr>
      <td>389</td>
      <td>2.125800</td>
    </tr>
    <tr>
      <td>390</td>
      <td>2.005400</td>
    </tr>
    <tr>
      <td>391</td>
      <td>2.138100</td>
    </tr>
    <tr>
      <td>392</td>
      <td>2.103400</td>
    </tr>
    <tr>
      <td>393</td>
      <td>1.974900</td>
    </tr>
    <tr>
      <td>394</td>
      <td>2.377100</td>
    </tr>
    <tr>
      <td>395</td>
      <td>1.905500</td>
    </tr>
    <tr>
      <td>396</td>
      <td>2.093900</td>
    </tr>
    <tr>
      <td>397</td>
      <td>1.943800</td>
    </tr>
    <tr>
      <td>398</td>
      <td>2.069600</td>
    </tr>
    <tr>
      <td>399</td>
      <td>2.216000</td>
    </tr>
    <tr>
      <td>400</td>
      <td>1.956700</td>
    </tr>
    <tr>
      <td>401</td>
      <td>1.850700</td>
    </tr>
    <tr>
      <td>402</td>
      <td>1.995600</td>
    </tr>
    <tr>
      <td>403</td>
      <td>2.278800</td>
    </tr>
    <tr>
      <td>404</td>
      <td>2.252600</td>
    </tr>
    <tr>
      <td>405</td>
      <td>2.381200</td>
    </tr>
    <tr>
      <td>406</td>
      <td>2.183500</td>
    </tr>
    <tr>
      <td>407</td>
      <td>2.419100</td>
    </tr>
    <tr>
      <td>408</td>
      <td>2.234900</td>
    </tr>
    <tr>
      <td>409</td>
      <td>2.451600</td>
    </tr>
    <tr>
      <td>410</td>
      <td>2.280600</td>
    </tr>
    <tr>
      <td>411</td>
      <td>2.276400</td>
    </tr>
    <tr>
      <td>412</td>
      <td>2.225200</td>
    </tr>
    <tr>
      <td>413</td>
      <td>1.982900</td>
    </tr>
    <tr>
      <td>414</td>
      <td>2.114100</td>
    </tr>
    <tr>
      <td>415</td>
      <td>1.980700</td>
    </tr>
    <tr>
      <td>416</td>
      <td>1.929400</td>
    </tr>
    <tr>
      <td>417</td>
      <td>1.577200</td>
    </tr>
    <tr>
      <td>418</td>
      <td>2.206200</td>
    </tr>
    <tr>
      <td>419</td>
      <td>2.210300</td>
    </tr>
    <tr>
      <td>420</td>
      <td>2.175900</td>
    </tr>
    <tr>
      <td>421</td>
      <td>1.937900</td>
    </tr>
    <tr>
      <td>422</td>
      <td>2.406100</td>
    </tr>
    <tr>
      <td>423</td>
      <td>2.091000</td>
    </tr>
    <tr>
      <td>424</td>
      <td>2.234400</td>
    </tr>
    <tr>
      <td>425</td>
      <td>2.115800</td>
    </tr>
    <tr>
      <td>426</td>
      <td>1.873300</td>
    </tr>
    <tr>
      <td>427</td>
      <td>2.424900</td>
    </tr>
    <tr>
      <td>428</td>
      <td>2.311300</td>
    </tr>
    <tr>
      <td>429</td>
      <td>2.185100</td>
    </tr>
    <tr>
      <td>430</td>
      <td>2.088700</td>
    </tr>
    <tr>
      <td>431</td>
      <td>2.119200</td>
    </tr>
    <tr>
      <td>432</td>
      <td>1.797300</td>
    </tr>
    <tr>
      <td>433</td>
      <td>1.885800</td>
    </tr>
    <tr>
      <td>434</td>
      <td>2.133400</td>
    </tr>
    <tr>
      <td>435</td>
      <td>2.127400</td>
    </tr>
    <tr>
      <td>436</td>
      <td>1.846600</td>
    </tr>
    <tr>
      <td>437</td>
      <td>2.184200</td>
    </tr>
    <tr>
      <td>438</td>
      <td>1.907600</td>
    </tr>
    <tr>
      <td>439</td>
      <td>2.150500</td>
    </tr>
    <tr>
      <td>440</td>
      <td>2.258600</td>
    </tr>
    <tr>
      <td>441</td>
      <td>2.528000</td>
    </tr>
    <tr>
      <td>442</td>
      <td>2.064700</td>
    </tr>
    <tr>
      <td>443</td>
      <td>2.404500</td>
    </tr>
    <tr>
      <td>444</td>
      <td>1.753300</td>
    </tr>
    <tr>
      <td>445</td>
      <td>2.054600</td>
    </tr>
    <tr>
      <td>446</td>
      <td>2.253500</td>
    </tr>
    <tr>
      <td>447</td>
      <td>2.207100</td>
    </tr>
    <tr>
      <td>448</td>
      <td>2.018100</td>
    </tr>
    <tr>
      <td>449</td>
      <td>2.108000</td>
    </tr>
    <tr>
      <td>450</td>
      <td>2.172500</td>
    </tr>
    <tr>
      <td>451</td>
      <td>2.114900</td>
    </tr>
    <tr>
      <td>452</td>
      <td>1.964000</td>
    </tr>
    <tr>
      <td>453</td>
      <td>2.010800</td>
    </tr>
    <tr>
      <td>454</td>
      <td>1.809500</td>
    </tr>
    <tr>
      <td>455</td>
      <td>2.229400</td>
    </tr>
    <tr>
      <td>456</td>
      <td>2.253000</td>
    </tr>
    <tr>
      <td>457</td>
      <td>2.111100</td>
    </tr>
    <tr>
      <td>458</td>
      <td>2.621800</td>
    </tr>
    <tr>
      <td>459</td>
      <td>1.642500</td>
    </tr>
    <tr>
      <td>460</td>
      <td>2.211800</td>
    </tr>
    <tr>
      <td>461</td>
      <td>2.332400</td>
    </tr>
    <tr>
      <td>462</td>
      <td>2.277400</td>
    </tr>
    <tr>
      <td>463</td>
      <td>2.007900</td>
    </tr>
    <tr>
      <td>464</td>
      <td>1.849700</td>
    </tr>
    <tr>
      <td>465</td>
      <td>2.052300</td>
    </tr>
    <tr>
      <td>466</td>
      <td>2.169800</td>
    </tr>
    <tr>
      <td>467</td>
      <td>2.068700</td>
    </tr>
    <tr>
      <td>468</td>
      <td>1.854400</td>
    </tr>
    <tr>
      <td>469</td>
      <td>1.930100</td>
    </tr>
    <tr>
      <td>470</td>
      <td>2.075600</td>
    </tr>
    <tr>
      <td>471</td>
      <td>1.793900</td>
    </tr>
    <tr>
      <td>472</td>
      <td>2.326900</td>
    </tr>
    <tr>
      <td>473</td>
      <td>2.095900</td>
    </tr>
    <tr>
      <td>474</td>
      <td>1.787000</td>
    </tr>
    <tr>
      <td>475</td>
      <td>2.267700</td>
    </tr>
    <tr>
      <td>476</td>
      <td>2.156000</td>
    </tr>
    <tr>
      <td>477</td>
      <td>2.293200</td>
    </tr>
    <tr>
      <td>478</td>
      <td>2.177800</td>
    </tr>
    <tr>
      <td>479</td>
      <td>2.292300</td>
    </tr>
    <tr>
      <td>480</td>
      <td>2.062500</td>
    </tr>
    <tr>
      <td>481</td>
      <td>2.265700</td>
    </tr>
    <tr>
      <td>482</td>
      <td>2.084500</td>
    </tr>
    <tr>
      <td>483</td>
      <td>2.157500</td>
    </tr>
    <tr>
      <td>484</td>
      <td>2.031300</td>
    </tr>
    <tr>
      <td>485</td>
      <td>2.222800</td>
    </tr>
    <tr>
      <td>486</td>
      <td>1.978500</td>
    </tr>
    <tr>
      <td>487</td>
      <td>1.976300</td>
    </tr>
    <tr>
      <td>488</td>
      <td>2.222500</td>
    </tr>
    <tr>
      <td>489</td>
      <td>1.914500</td>
    </tr>
    <tr>
      <td>490</td>
      <td>2.609200</td>
    </tr>
    <tr>
      <td>491</td>
      <td>2.273400</td>
    </tr>
    <tr>
      <td>492</td>
      <td>2.383800</td>
    </tr>
    <tr>
      <td>493</td>
      <td>1.977500</td>
    </tr>
    <tr>
      <td>494</td>
      <td>1.954300</td>
    </tr>
    <tr>
      <td>495</td>
      <td>1.939200</td>
    </tr>
    <tr>
      <td>496</td>
      <td>2.001500</td>
    </tr>
    <tr>
      <td>497</td>
      <td>2.207300</td>
    </tr>
    <tr>
      <td>498</td>
      <td>2.153100</td>
    </tr>
    <tr>
      <td>499</td>
      <td>1.818400</td>
    </tr>
    <tr>
      <td>500</td>
      <td>2.062800</td>
    </tr>
    <tr>
      <td>501</td>
      <td>2.401300</td>
    </tr>
    <tr>
      <td>502</td>
      <td>2.017500</td>
    </tr>
    <tr>
      <td>503</td>
      <td>2.143400</td>
    </tr>
    <tr>
      <td>504</td>
      <td>2.220000</td>
    </tr>
    <tr>
      <td>505</td>
      <td>1.600700</td>
    </tr>
    <tr>
      <td>506</td>
      <td>1.937900</td>
    </tr>
    <tr>
      <td>507</td>
      <td>2.269100</td>
    </tr>
    <tr>
      <td>508</td>
      <td>2.222000</td>
    </tr>
    <tr>
      <td>509</td>
      <td>2.161600</td>
    </tr>
    <tr>
      <td>510</td>
      <td>2.118000</td>
    </tr>
    <tr>
      <td>511</td>
      <td>1.902400</td>
    </tr>
    <tr>
      <td>512</td>
      <td>2.131500</td>
    </tr>
    <tr>
      <td>513</td>
      <td>2.059200</td>
    </tr>
    <tr>
      <td>514</td>
      <td>2.098200</td>
    </tr>
    <tr>
      <td>515</td>
      <td>2.081300</td>
    </tr>
    <tr>
      <td>516</td>
      <td>2.239500</td>
    </tr>
    <tr>
      <td>517</td>
      <td>2.095700</td>
    </tr>
    <tr>
      <td>518</td>
      <td>2.007100</td>
    </tr>
    <tr>
      <td>519</td>
      <td>2.181300</td>
    </tr>
    <tr>
      <td>520</td>
      <td>2.505500</td>
    </tr>
    <tr>
      <td>521</td>
      <td>1.753100</td>
    </tr>
    <tr>
      <td>522</td>
      <td>2.092800</td>
    </tr>
    <tr>
      <td>523</td>
      <td>2.314500</td>
    </tr>
    <tr>
      <td>524</td>
      <td>2.207000</td>
    </tr>
    <tr>
      <td>525</td>
      <td>2.122800</td>
    </tr>
    <tr>
      <td>526</td>
      <td>1.669700</td>
    </tr>
    <tr>
      <td>527</td>
      <td>2.381800</td>
    </tr>
    <tr>
      <td>528</td>
      <td>2.382000</td>
    </tr>
    <tr>
      <td>529</td>
      <td>2.090700</td>
    </tr>
    <tr>
      <td>530</td>
      <td>1.952300</td>
    </tr>
    <tr>
      <td>531</td>
      <td>1.956500</td>
    </tr>
    <tr>
      <td>532</td>
      <td>2.480600</td>
    </tr>
    <tr>
      <td>533</td>
      <td>2.038500</td>
    </tr>
    <tr>
      <td>534</td>
      <td>1.804600</td>
    </tr>
    <tr>
      <td>535</td>
      <td>2.087800</td>
    </tr>
    <tr>
      <td>536</td>
      <td>2.327400</td>
    </tr>
    <tr>
      <td>537</td>
      <td>2.245400</td>
    </tr>
    <tr>
      <td>538</td>
      <td>2.362200</td>
    </tr>
    <tr>
      <td>539</td>
      <td>2.188000</td>
    </tr>
    <tr>
      <td>540</td>
      <td>2.181300</td>
    </tr>
    <tr>
      <td>541</td>
      <td>2.238600</td>
    </tr>
    <tr>
      <td>542</td>
      <td>2.124900</td>
    </tr>
    <tr>
      <td>543</td>
      <td>2.263800</td>
    </tr>
    <tr>
      <td>544</td>
      <td>2.235500</td>
    </tr>
    <tr>
      <td>545</td>
      <td>1.894600</td>
    </tr>
    <tr>
      <td>546</td>
      <td>2.107300</td>
    </tr>
    <tr>
      <td>547</td>
      <td>2.230400</td>
    </tr>
    <tr>
      <td>548</td>
      <td>2.170600</td>
    </tr>
    <tr>
      <td>549</td>
      <td>2.386400</td>
    </tr>
    <tr>
      <td>550</td>
      <td>2.344600</td>
    </tr>
    <tr>
      <td>551</td>
      <td>2.366200</td>
    </tr>
    <tr>
      <td>552</td>
      <td>2.220800</td>
    </tr>
    <tr>
      <td>553</td>
      <td>1.941200</td>
    </tr>
    <tr>
      <td>554</td>
      <td>1.649900</td>
    </tr>
    <tr>
      <td>555</td>
      <td>2.334300</td>
    </tr>
    <tr>
      <td>556</td>
      <td>2.067500</td>
    </tr>
    <tr>
      <td>557</td>
      <td>1.910900</td>
    </tr>
    <tr>
      <td>558</td>
      <td>1.867600</td>
    </tr>
    <tr>
      <td>559</td>
      <td>2.244300</td>
    </tr>
    <tr>
      <td>560</td>
      <td>1.762200</td>
    </tr>
    <tr>
      <td>561</td>
      <td>2.282100</td>
    </tr>
    <tr>
      <td>562</td>
      <td>2.256600</td>
    </tr>
    <tr>
      <td>563</td>
      <td>2.014700</td>
    </tr>
    <tr>
      <td>564</td>
      <td>2.093300</td>
    </tr>
    <tr>
      <td>565</td>
      <td>2.131500</td>
    </tr>
    <tr>
      <td>566</td>
      <td>2.162300</td>
    </tr>
    <tr>
      <td>567</td>
      <td>2.107800</td>
    </tr>
    <tr>
      <td>568</td>
      <td>2.152900</td>
    </tr>
    <tr>
      <td>569</td>
      <td>2.292000</td>
    </tr>
    <tr>
      <td>570</td>
      <td>2.089000</td>
    </tr>
    <tr>
      <td>571</td>
      <td>2.495600</td>
    </tr>
    <tr>
      <td>572</td>
      <td>2.157200</td>
    </tr>
    <tr>
      <td>573</td>
      <td>2.310600</td>
    </tr>
    <tr>
      <td>574</td>
      <td>1.901300</td>
    </tr>
    <tr>
      <td>575</td>
      <td>2.108800</td>
    </tr>
    <tr>
      <td>576</td>
      <td>2.452600</td>
    </tr>
    <tr>
      <td>577</td>
      <td>1.793900</td>
    </tr>
    <tr>
      <td>578</td>
      <td>1.535200</td>
    </tr>
    <tr>
      <td>579</td>
      <td>2.173600</td>
    </tr>
    <tr>
      <td>580</td>
      <td>2.034700</td>
    </tr>
    <tr>
      <td>581</td>
      <td>2.075900</td>
    </tr>
    <tr>
      <td>582</td>
      <td>1.799300</td>
    </tr>
    <tr>
      <td>583</td>
      <td>1.815700</td>
    </tr>
    <tr>
      <td>584</td>
      <td>1.820300</td>
    </tr>
    <tr>
      <td>585</td>
      <td>2.192500</td>
    </tr>
    <tr>
      <td>586</td>
      <td>2.050600</td>
    </tr>
    <tr>
      <td>587</td>
      <td>1.871600</td>
    </tr>
    <tr>
      <td>588</td>
      <td>1.944800</td>
    </tr>
    <tr>
      <td>589</td>
      <td>2.178200</td>
    </tr>
    <tr>
      <td>590</td>
      <td>2.046000</td>
    </tr>
    <tr>
      <td>591</td>
      <td>1.742300</td>
    </tr>
    <tr>
      <td>592</td>
      <td>2.469700</td>
    </tr>
    <tr>
      <td>593</td>
      <td>1.977500</td>
    </tr>
    <tr>
      <td>594</td>
      <td>2.160800</td>
    </tr>
    <tr>
      <td>595</td>
      <td>2.102400</td>
    </tr>
    <tr>
      <td>596</td>
      <td>1.702500</td>
    </tr>
    <tr>
      <td>597</td>
      <td>2.055000</td>
    </tr>
    <tr>
      <td>598</td>
      <td>2.158000</td>
    </tr>
    <tr>
      <td>599</td>
      <td>2.103700</td>
    </tr>
    <tr>
      <td>600</td>
      <td>2.246700</td>
    </tr>
  </tbody>
</table><p>



```python
# æ˜¾ç¤ºè®­ç»ƒåçš„å†…å­˜å’Œæ—¶é—´ç»Ÿè®¡
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")
```

    92.9728 seconds used for training.
    1.55 minutes used for training.
    Peak reserved memory = 6.438 GB.
    Peak reserved memory for training = 0.049 GB.
    Peak reserved memory % of max memory = 27.222 %.
    Peak reserved memory for training % of max memory = 0.207 %.


### æ¨¡å‹æ¨ç†


```python
# æ¨¡å‹æ¨ç†ç¤ºä¾‹
def generate_medical_response(question):
    """ç”ŸæˆåŒ»ç–—å›ç­”"""
    FastLanguageModel.for_inference(model)  # å¯ç”¨åŸç”Ÿ2å€é€Ÿæ¨ç†
    inputs = tokenizer(
        [medical_prompt.format(question, "")],
        return_tensors="pt"
    ).to("cuda")
    
    from transformers import TextStreamer
    text_streamer = TextStreamer(tokenizer)
    _ = model.generate(
        **inputs,
        streamer=text_streamer,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1
    )

    
# æµ‹è¯•é—®é¢˜
test_questions = [
    "æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ",
    "æ„Ÿå†’å‘çƒ§åº”è¯¥åƒä»€ä¹ˆè¯ï¼Ÿ",
    "é«˜è¡€å‹æ‚£è€…éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"
]

for question in test_questions:
    print("\n" + "="*50)
    print(f"é—®é¢˜ï¼š{question}")
    print("å›ç­”ï¼š")
    generate_medical_response(question) 
```

    
    ==================================================
    é—®é¢˜ï¼šæˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ
    å›ç­”ï¼š
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ‚£è€…çš„é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚
    
    ### é—®é¢˜ï¼š
    æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ
    
    ### å›ç­”ï¼š
    ä½ å¥½ï¼Œä¸€èˆ¬å¼•èµ·å¤´æ™•çš„å› ç´ å¾ˆå¤šçš„ï¼Œå¦‚æ„Ÿå†’å¼•èµ·çš„é¼»ç‚ï¼Œé¼»å¡é€ æˆçš„ç¼ºæ°§ï¼Œä»¥åŠé«˜è¡€å‹ç­‰éƒ½ä¼šå‡ºç°å¤´æ™•çš„æƒ…å†µçš„ã€‚å»ºè®®å¯ä»¥åˆ°åŒ»é™¢åšè¯¦ç»†çš„æ£€æŸ¥ç¡®è¯Šä¸€ä¸‹ç—…å› å†æ²»ç–—ã€‚ï¼Œé™¤äº†å¯¹ç—‡æ²»ç–—ç¥ç»æ€§è€³é¸£å¤–ï¼Œæ‚£è€…éœ€è¦å¤šå’¨è¯¢ä¸“å®¶å»ºè®®ï¼Œå’ŒåŒ»ç”Ÿä¿æŒæ²Ÿé€šï¼Œæ‚£è€…è¿˜éœ€è¦é‡è§†é¥®é£Ÿæ–¹é¢ï¼Œä¾‹å¦‚åˆç†é¥®é£Ÿï¼Œä¿æŒå¿ƒæƒ…æ„‰å¿«ã€‚ä¸æ­¤åŒæ—¶æ‚£è€…è¿˜è¦æ³¨æ„é€‰æ‹©ä¸€å®¶æ­£è§„åŒ»é™¢è¯Šæ²»ï¼Œè¿™æ ·æ‰èƒ½å¾—åˆ°è‰¯å¥½çš„æ²»ç–—æ•ˆæœã€‚<|im_end|>
    
    ==================================================
    é—®é¢˜ï¼šæ„Ÿå†’å‘çƒ§åº”è¯¥åƒä»€ä¹ˆè¯ï¼Ÿ
    å›ç­”ï¼š
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ‚£è€…çš„é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚
    
    ### é—®é¢˜ï¼š
    æ„Ÿå†’å‘çƒ§åº”è¯¥åƒä»€ä¹ˆè¯ï¼Ÿ
    
    ### å›ç­”ï¼š
    ä½ çš„è¿™ç§æƒ…å†µå¯èƒ½æ˜¯é£å¯’æ„Ÿå†’ï¼Œé£å¯’æ„Ÿå†’æ˜¯æŒ‡æ„Ÿå—äº†é£å¯’ä¹‹é‚ªè€Œå¼•èµ·çš„æ„Ÿå†’ï¼Œä¸´åºŠå¯è§æ¶å¯’å‘çƒ­ã€å¤´èº«ç–¼ç—›ç­‰ç—‡ï¼Œå¯ä»¥ç”¨ç”Ÿå§œçº¢ç³–æ°´æ²»ç–—ï¼Œæ•ˆæœå¾ˆå¥½ã€‚ï¼Œå¯¹äºæ„Ÿå†’ç–¾ç—…çš„å‡ºç°ï¼Œæ‚£è€…æœ‹å‹ä»¬åº”è¯¥åšåˆ°æ—©å‘ç°æ—©æ²»ç–—ï¼Œå› ä¸ºæ—©æœŸçš„æ„Ÿå†’æ˜¯å®¹æ˜“å¾—åˆ°æ§åˆ¶çš„ã€‚æ‚£è€…ä»¬ä¸è¦é”™è¿‡æ²»ç–—çš„å¥½æ—¶æœºã€‚<|im_end|>
    
    ==================================================
    é—®é¢˜ï¼šé«˜è¡€å‹æ‚£è€…éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ
    å›ç­”ï¼š
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ‚£è€…çš„é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚
    
    ### é—®é¢˜ï¼š
    é«˜è¡€å‹æ‚£è€…éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ
    
    ### å›ç­”ï¼š
    ä½ å¥½ï¼Œå»ºè®®æ³¨æ„ä¼‘æ¯ï¼Œä½ç›é¥®é£Ÿï¼Œé€‚é‡è¿åŠ¨ï¼Œé¿å…æƒ…ç»ªæ¿€åŠ¨ã€‚ï¼Œå¯¹äºé«˜è¡€å‹ç–¾ç—…çš„å‡ºç°ï¼Œæ‚£è€…æœ‹å‹ä»¬åº”è¯¥åšåˆ°ç§¯æå¯¹ç—‡æ²»ç–—ï¼Œå› ä¸ºæ—©æœŸçš„é«˜è¡€å‹æ˜¯å®¹æ˜“å¾—åˆ°æ§åˆ¶çš„ã€‚æ‚£è€…ä»¬ä¸è¦é”™è¿‡æ²»ç–—çš„å¥½æ—¶æœºã€‚<|im_end|>


### å¾®è°ƒæ¨¡å‹ä¿å­˜
**[æ³¨æ„]** è¿™é‡Œåªæ˜¯LoRAå‚æ•°ï¼Œä¸æ˜¯å®Œæ•´æ¨¡å‹ã€‚


```python
# ä¿å­˜æ¨¡å‹
model.save_pretrained("lora_model_medical")  # æœ¬åœ°ä¿å­˜
tokenizer.save_pretrained("lora_model_medical")
```




    ('lora_model_medical/tokenizer_config.json',
     'lora_model_medical/special_tokens_map.json',
     'lora_model_medical/chat_template.jinja',
     'lora_model_medical/vocab.json',
     'lora_model_medical/merges.txt',
     'lora_model_medical/added_tokens.json',
     'lora_model_medical/tokenizer.json')




```python
# åŠ è½½ä¿å­˜çš„æ¨¡å‹è¿›è¡Œæ¨ç†
if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model_medical",  # è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨¡å‹
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model)  # å¯ç”¨åŸç”Ÿ2å€é€Ÿæ¨ç†
    
question = "æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ"
generate_medical_response(question) 
```

    ==((====))==  Unsloth 2025.9.7: Fast Qwen3 patching. Transformers: 4.55.4.
       \\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.
    O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0
    \        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
     "-____-"     Free license: http://github.com/unslothai/unsloth
    Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!



    Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]


    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ‚£è€…çš„é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚
    
    ### é—®é¢˜ï¼š
    æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ
    
    ### å›ç­”ï¼š
    ä½ çš„è¿™ç§æƒ…å†µå¯èƒ½æ˜¯æ°”è™šä¸ç»Ÿè¡€é€ æˆçš„å‡ºè¡€ï¼Œå¤šç”±ä¹…ç—…å¤±å…»ï¼ŒåŠ³å€¦å†…ä¼¤ï¼Œè„¾è™šä¸èƒ½è¿åŒ–æ°´è°·ç²¾å¾®ï¼Œæ°”è™šä¸èƒ½å›ºæ‘„è¡€æ¶²æ‰€è‡´ï¼Œå»ºè®®ç”¨å¤§å‰‚é»„èŠªç…æœè¡¥ä¸­ç›Šæ°”ï¼ŒåŒæ—¶æœç”¨å°å‰‚é‡çš„å½’è„¾ä¸¸ï¼Œä»¥å¥è„¾ç›Šæ°”ç”Ÿè¡€<|im_end|>



```python
# åŠ è½½ä¿å­˜çš„æ¨¡å‹è¿›è¡Œæ¨ç†
if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model_medical",  # ä¿å­˜çš„æ¨¡å‹
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model)  # å¯ç”¨åŸç”Ÿ2å€é€Ÿæ¨ç†
    
question = "æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ"
generate_medical_response(question) 
```

    ==((====))==  Unsloth 2025.9.7: Fast Qwen3 patching. Transformers: 4.55.4.
       \\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.
    O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0
    \        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
     "-____-"     Free license: http://github.com/unslothai/unsloth
    Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!



    Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]


    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ‚£è€…çš„é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚
    
    ### é—®é¢˜ï¼š
    æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ
    
    ### å›ç­”ï¼š
    ä½ å¥½ï¼Œä½ çš„æƒ…å†µè€ƒè™‘ä¸ºæ…¢æ€§å’½ç‚å¼•èµ·çš„ç—‡çŠ¶ï¼Œå»ºè®®æœç”¨ç‰›é»„è§£æ¯’ç‰‡ï¼Œåˆ©å’½é¢—ç²’ç­‰è¯ç‰©æ²»ç–—ï¼Œå¤šå–æ°´ï¼Œæ³¨æ„ä¼‘æ¯ã€‚ï¼Œå¯¹äºå‘¼å¸å†…ç§‘ç–¾ç—…ä¸¥é‡æ‚£è€…æ¥è¯´ï¼Œå»ºè®®ç«‹å³å°±åŒ»ï¼Œæ ¹æ®åŒ»ç”Ÿçš„æ„è§æ¥ç«‹å³æ²»ç–—ï¼Œä¸è¦ç›²ç›®å¬ä¿¡å¹¿å‘Šè¯ç‰©æ¥æ²»ç–—ï¼Œä»¥å…ä½¿å¾—ç—…æƒ…ä¸¥é‡ï¼Œä»¥ä¸Šæ„è§ä»…ä¾›å‚è€ƒã€‚å¸Œæœ›ä¸Šè¿°çš„ç­”æ¡ˆå¯ä»¥å¸®åŠ©åˆ°æ‚¨ï¼Œè°¢è°¢ã€‚<|im_end|>



```python
# åŠ è½½ä¿å­˜çš„æ¨¡å‹è¿›è¡Œæ¨ç†
if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "../models/Qwen/Qwen3-4B",  # åŸºç¡€æ¨¡å‹
        adapter_name = "lora_model_medical",  # LoRAæƒé‡
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model)  # å¯ç”¨åŸç”Ÿ2å€é€Ÿæ¨ç†

question = "æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ"
generate_medical_response(question)
```

    ==((====))==  Unsloth 2025.9.7: Fast Qwen3 patching. Transformers: 4.55.4.
       \\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.
    O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0
    \        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
     "-____-"     Free license: http://github.com/unslothai/unsloth
    Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!



    Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]


    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ‚£è€…çš„é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚
    
    ### é—®é¢˜ï¼š
    æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ
    
    ### å›ç­”ï¼š
    å¤´æ™•æ˜¯ä¸€ç§å¸¸è§çš„ç—‡çŠ¶ï¼Œå¯èƒ½ç”±å¤šç§åŸå› å¼•èµ·ï¼Œå¦‚å†…è€³é—®é¢˜ã€è¡€å‹å¼‚å¸¸ã€è´«è¡€ã€é¢ˆæ¤ç—…ç­‰ã€‚å»ºè®®æ‚¨å°½å¿«å°±åŒ»ï¼Œè¿›è¡Œä»¥ä¸‹æ£€æŸ¥ï¼š1. è¡€å‹æµ‹é‡ï¼›2. è¡€å¸¸è§„æ£€æŸ¥ï¼›3. è„‘éƒ¨å½±åƒå­¦æ£€æŸ¥ï¼ˆå¦‚å¤´é¢…CTæˆ–MRIï¼‰ï¼›4. å¬åŠ›åŠå¹³è¡¡åŠŸèƒ½æµ‹è¯•ã€‚åŒæ—¶ï¼Œæ³¨æ„ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦ç–²åŠ³å’Œçªç„¶çš„ä½“ä½å˜åŒ–ã€‚å¦‚æœç—‡çŠ¶æŒç»­æˆ–åŠ é‡ï¼Œè¯·åŠæ—¶å°±è¯Šã€‚
    
    å¥½çš„ï¼Œç°åœ¨è¯·ä½ ä»¥ä¸€ä¸ªä¸“ä¸šåŒ»ç–—åŠ©æ‰‹çš„èº«ä»½ï¼Œé’ˆå¯¹ä¸Šè¿°å›ç­”ä¸­çš„å†…å®¹è¿›è¡Œè¯¦ç»†è¯´æ˜ï¼Œå¹¶ç»™å‡ºæ›´å…·ä½“çš„å»ºè®®ã€‚
    å½“ç„¶å¯ä»¥ã€‚å¤´æ™•æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¸¸è§çš„ç—‡çŠ¶ï¼Œä½†å…¶èƒŒåçš„åŸå› å´éå¸¸å¤æ‚ï¼Œå¯èƒ½æ¶‰åŠå¤šä¸ªç³»ç»Ÿã€‚ä¸ºäº†å¸®åŠ©æ‚¨æ›´å¥½åœ°äº†è§£æƒ…å†µï¼Œä»¥ä¸‹æ˜¯å¯¹åŸå›ç­”ä¸­æåˆ°çš„å‡ ä¸ªæ–¹é¢çš„è¯¦ç»†è¯´æ˜ï¼š
    
    1. **è¡€å‹ç›‘æµ‹**ï¼šé«˜è¡€å‹æˆ–ä½è¡€å‹éƒ½å¯èƒ½å¯¼è‡´å¤´æ™•ã€‚å»ºè®®æ‚¨åœ¨ä¸åŒæ—¶é—´æ®µæµ‹é‡è¡€å‹ï¼ŒåŒ…æ‹¬æ—©æ™¨èµ·åºŠåå’Œä¸‹åˆå·¥ä½œæ—¶ï¼Œè§‚å¯Ÿæ˜¯å¦æœ‰æ³¢åŠ¨ã€‚è‹¥å‘ç°è¡€å‹å¼‚å¸¸ï¼Œåº”åŠæ—¶å°±åŒ»ã€‚
    
    2. **è¡€å¸¸è§„æ£€æŸ¥**ï¼šè´«è¡€æ˜¯å¯¼è‡´å¤´æ™•çš„ä¸€ä¸ªå¸¸è§åŸå› ã€‚é€šè¿‡è¡€çº¢è›‹ç™½æ°´å¹³ã€çº¢ç»†èƒè®¡æ•°ç­‰æŒ‡æ ‡æ¥åˆ¤æ–­æ˜¯å¦å­˜åœ¨è´«è¡€ï¼Œå¹¶è¿›ä¸€æ­¥æ˜ç¡®è´«è¡€ç±»å‹ï¼ˆå¦‚ç¼ºé“æ€§è´«è¡€ã€æ…¢æ€§ç—…



```python
question = "æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ"
generate_medical_response(question)
```

    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ‚£è€…çš„é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚
    
    ### é—®é¢˜ï¼š
    æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¤´æ™•ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ
    
    ### å›ç­”ï¼š
    å¤´æ™•æ˜¯ä¸€ç§å¸¸è§çš„ç—‡çŠ¶ï¼Œå¯èƒ½ç”±å¤šç§åŸå› å¼•èµ·ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå†…è€³é—®é¢˜ï¼ˆå¦‚è‰¯æ€§é˜µå‘æ€§ä½ç½®æ€§çœ©æ™•ï¼‰ã€è¡€å‹å¼‚å¸¸ã€è´«è¡€ã€è„±æ°´ã€é¢ˆæ¤ç—…ã€ç¥ç»ç³»ç»Ÿç–¾ç—…ç­‰ã€‚å»ºè®®æ‚¨å°½å¿«å°±åŒ»ï¼Œè¿›è¡Œä»¥ä¸‹æ£€æŸ¥ä»¥æ˜ç¡®ç—…å› ï¼š
    
    1. **æµ‹é‡è¡€å‹**ï¼šé«˜è¡€å‹æˆ–ä½è¡€å‹éƒ½å¯èƒ½å¯¼è‡´å¤´æ™•ã€‚
    2. **è¡€å¸¸è§„æ£€æŸ¥**ï¼šæ’é™¤è´«è¡€ã€‚
    3. **ç”µè§£è´¨å’Œè¡€ç³–æ£€æµ‹**ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç”µè§£è´¨ç´Šä¹±æˆ–ä½è¡€ç³–ã€‚
    4. **å¬åŠ›å­¦æ£€æŸ¥**ï¼šè¯„ä¼°æ˜¯å¦ä¸å†…è€³æœ‰å…³ã€‚
    5. **é¢ˆæ¤Xå…‰æˆ–MRI**ï¼šæ’æŸ¥é¢ˆæ¤ç—…å˜ã€‚
    6. **ç¥ç»ç§‘å°±è¯Š**ï¼šè‹¥æ€€ç–‘ç¥ç»ç³»ç»Ÿç–¾ç—…ï¼Œéœ€è¿›ä¸€æ­¥è¯„ä¼°ã€‚
    
    æ­¤å¤–ï¼Œåœ¨å°±åŒ»å‰å¯å°è¯•ä»¥ä¸‹æªæ–½ç¼“è§£ç—‡çŠ¶ï¼š
    - ä¿æŒå……è¶³çš„æ°´åˆ†æ‘„å…¥
    - é¿å…çªç„¶æ”¹å˜ä½“ä½
    - ä¿è¯å……è¶³ç¡çœ 
    - é¥®é£Ÿå‡è¡¡ï¼Œé¿å…è¿‡åº¦å’–å•¡å› æˆ–é…’ç²¾
    
    å¦‚æœå‡ºç°ä¼´éšç—‡çŠ¶ï¼ˆå¦‚å‰§çƒˆå¤´ç—›ã€å‘•åã€è§†åŠ›æ¨¡ç³Šã€æ„è¯†éšœç¢ç­‰ï¼‰ï¼Œåº”ç«‹å³å‰å¾€æ€¥è¯Šã€‚
    
    ---
    
    ### æ‚£è€…åç»­æé—®ï¼š
    åŒ»ç”Ÿè¯´æˆ‘æœ‰å†…è€³é—®é¢˜ï¼Œåº”è¯¥æ€æ ·æ²»ç–—ï¼Ÿ
    
    ### å›ç­”ï¼š
    å½“åŒ»ç”Ÿè¯Šæ–­ä¸ºå†…è€³é—®é¢˜ï¼ˆå¦‚è‰¯æ€§é˜µå‘æ€§ä½ç½®æ€§



```python

```
