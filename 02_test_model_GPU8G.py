"""
è¿™ä¸ªæ–‡ä»¶çš„å­˜åœ¨æ˜¯å› ä¸ºæˆ‘æœ¬åœ°æ˜¯8G GPU,ç›®æ ‡æ˜¯éœ€è¦16G GPUã€‚
å¦‚æœä½ çš„ç¯å¢ƒæœ‰16G GPUï¼Œå¯ä»¥ç›´æ¥è¿è¡Œè¿™ä¸ªæ–‡ä»¶ï¼š02_test_model_GPU16G.py
"""

"""
æµ‹è¯•ç”¨ä¾‹ï¼š

=== åŒ»ç–—é—®ç­”ç³»ç»Ÿ (8GB GPUä¼˜åŒ–ç‰ˆ) ===
åŸºç¡€æ¨¡å‹è·¯å¾„: models/Qwen/Qwen3-4B
LoRAé€‚é…å™¨è·¯å¾„: models/Sogrey/Chinese-medical-lora
ğŸ› ï¸ æ­£åœ¨ä¸º8GB GPUä¼˜åŒ–åŠ è½½æ¨¡å‹...
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:27<00:00,  9.24s/it]
âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å½“å‰æ˜¾å­˜ä½¿ç”¨ï¼š2.61GB

æ‚£è€…æé—®: è¯·é—®æ…¢æ€§è‚¾ç‚æ‚£è€…èƒ½åƒè±†è…å—ï¼Ÿ

AIåŒ»ç”Ÿæ€è€ƒä¸­...è¯·é—®æ…¢æ€§è‚¾ç‚æ‚£è€…èƒ½åƒè±†è…å—ï¼Ÿ

### å›ç­”ï¼š
ä½ å¥½ï¼Œè‚¾è„ç–¾ç—…æ˜¯ä¸´åºŠå¸¸è§çš„ç—…ç—‡ï¼Œå…¶ä¸»è¦è¡¨ç°ä¸ºè›‹ç™½å°¿ã€è¡€å°¿ã€æ°´è‚¿ã€é«˜è¡€å‹ç­‰ã€‚ä¸€èˆ¬éœ€è¦é€šè¿‡é¥®é£Ÿæ²»ç–—å’Œè¯ç‰©æ²»ç–—ç›¸ç»“åˆçš„æ–¹æ³• è¿›è¡Œæ²»ç–—çš„ã€‚å»ºè®®å¹³æ—¶æ³¨æ„ä¼‘æ¯ï¼Œé¿å…åŠ³ç´¯ï¼Œç¦çƒŸé…’ï¼Œä½ç›ä½è„‚é¥®é£Ÿï¼Œé«˜çƒ­é‡ä¼˜è´¨è›‹ç™½è´¨é¥®é£Ÿï¼Œå¿Œé£Ÿè¾›è¾£åˆºæ¿€æ€§é£Ÿç‰©ã€‚ï¼Œå¯¹äºè‚¾ç‚ç–¾ ç—…çš„å‡ºç°ï¼Œæ‚£è€…æœ‹å‹ä»¬åº”è¯¥åšåˆ°ç§¯æå¯¹ç—‡æ²»ç–—ï¼Œå› ä¸ºæ—©æœŸçš„è‚¾ç‚æ˜¯å®¹æ˜“å¾—åˆ°æ§åˆ¶çš„ã€‚æ‚£è€…ä»¬ä¸è¦é”™è¿‡æ²»ç–—çš„å¥½æ—¶æœºã€‚
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig
from peft import PeftModel
import os
import gc
from datetime import datetime

# æ¨¡å‹è·¯å¾„é…ç½®
BASE_MODEL = "models/Qwen/Qwen3-4B"
LORA_MODEL = "models/Sogrey/Chinese-medical-lora"

class MedicalQA_8GB_Optimized:
    def __init__(self):
        """åˆå§‹åŒ–åŒ»ç–—QAç³»ç»Ÿï¼ˆ8GB GPUä¼˜åŒ–ç‰ˆï¼‰"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.offload_dir = f"./offload_{datetime.now().strftime('%Y%m%d')}"
        self._setup_environment()
        
        # æå‰åˆå§‹åŒ–tokenizerï¼ˆä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼‰
        self.tokenizer = AutoTokenizer.from_pretrained(
            BASE_MODEL,
            trust_remote_code=True,
            pad_token='<|extra_0|>'  # è®¾ç½®æ˜ç¡®çš„pad_token
        )
        self.model = None
        self._load_models()

    def _setup_environment(self):
        """é…ç½®è¿è¡Œç¯å¢ƒ"""
        os.makedirs(self.offload_dir, exist_ok=True)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.set_grad_enabled(False)
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def _load_models(self):
        """åŠ è½½é‡åŒ–æ¨¡å‹å’ŒLoRAé€‚é…å™¨"""
        print("ğŸ› ï¸ æ­£åœ¨ä¸º8GB GPUä¼˜åŒ–åŠ è½½æ¨¡å‹...")
        
        try:
            # é‡åŒ–é…ç½®ï¼ˆå…¼å®¹8GBæ˜¾å­˜ï¼‰
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True
            )

            # æ˜¾å­˜åˆ†é…ç­–ç•¥ï¼ˆå…³é”®é…ç½®ï¼‰
            memory_mapping = {
                0: "6GiB",    # ä¸»GPUä¿ç•™6GB
                "cpu": "24GiB" # ç³»ç»Ÿå†…å­˜åˆ†é…24GB
            }

            # 1. åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆ4ä½é‡åŒ–ï¼‰
            base_model = AutoModelForCausalLM.from_pretrained(
                BASE_MODEL,
                device_map="auto",
                quantization_config=bnb_config,
                offload_folder=self.offload_dir,
                max_memory=memory_mapping,
                torch_dtype=torch.float16,
                trust_remote_code=True
            )

            # 2. åŠ è½½LoRAé€‚é…å™¨
            self.model = PeftModel.from_pretrained(
                base_model,
                LORA_MODEL,
                device_map="auto",
                offload_folder=self.offload_dir,
                max_memory=memory_mapping
            ).eval()

            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å½“å‰æ˜¾å­˜ä½¿ç”¨ï¼š{torch.cuda.memory_allocated()/1024**3:.2f}GB")
            
        except Exception as e:
            self._cleanup()
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

    def stream_answer(self, question, max_length=512):
        """æµå¼ç”Ÿæˆå›ç­”"""
        try:
            inputs = self.tokenizer(question, return_tensors="pt", padding=True).to(self.device)
            
            # æ˜¾å¼è®¾ç½®Qwenæ¨¡å‹ç‰¹å®šçš„ç”Ÿæˆé…ç½®
            generation_config = GenerationConfig(
                max_length=max_length,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=20,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                bos_token_id=151643,  # Qwenç‰¹å®šçš„BOS token
                forced_eos_token_id=151645  # Qwenç‰¹å®šçš„EOS token
            )
            
            # åˆ›å»ºç”Ÿæˆå™¨æ—¶ç¦æ­¢è°ƒè¯•ä¿¡æ¯
            with torch.no_grad():
                output_stream = self.model.generate(
                    **inputs,
                    generation_config=generation_config,
                    streamer=None,
                    return_dict_in_generate=True,
                    output_scores=True
                )
            
            # æ¨¡æ‹Ÿæµå¼è¾“å‡º
            generated_text = ""
            for seq in output_stream.sequences:
                new_text = self.tokenizer.decode(seq, skip_special_tokens=True)
                if new_text.startswith(question):
                    new_text = new_text[len(question):].strip()
                if new_text and new_text != generated_text:
                    yield new_text[len(generated_text):]
                    generated_text = new_text
                    
        except Exception as e:
            raise RuntimeError(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

    def _cleanup(self):
        """å®‰å…¨çš„èµ„æºæ¸…ç†"""
        try:
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        except:
            pass

    def __del__(self):
        self._cleanup()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    try:
        print("=== åŒ»ç–—é—®ç­”ç³»ç»Ÿ (8GB GPUä¼˜åŒ–ç‰ˆ) ===")
        print(f"åŸºç¡€æ¨¡å‹è·¯å¾„: {BASE_MODEL}")
        print(f"LoRAé€‚é…å™¨è·¯å¾„: {LORA_MODEL}")
        
        qa = MedicalQA_8GB_Optimized()
        
        while True:
            question = input("\næ‚£è€…æé—®: ").strip()
            if question.lower() in ['exit', 'quit', 'q']:
                break
                
            print("\nAIåŒ»ç”Ÿæ€è€ƒä¸­...\n", end="", flush=True)
            
            try:
                # æµå¼è¾“å‡º
                full_answer = ""
                for chunk in qa.stream_answer(question):
                    print(chunk, end="", flush=True)
                    full_answer += chunk
                
                print("\n")  # å›ç­”ç»“æŸåæ¢è¡Œ
                
            except Exception as e:
                print(f"\nâŒ ç”Ÿæˆé”™è¯¯: {str(e)}")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å·²é€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {str(e)}")
    finally:
        if 'qa' in locals():
            qa._cleanup()
